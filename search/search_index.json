{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"DataJoint Elements This resource (\"Resource\") provides an efficient approach for neuroscience labs to create and manage scientific data workflows : the complex multi-step methods for data collection, preparation, processing, analysis, and modeling that researchers must perform in the course of an experimental study. The work is derived from the developments in leading neuroscience projects and uses the DataJoint framework for defining, deploying, and sharing their data workflows. An overview of the principles of DataJoint workflows and the goals of DataJoint Elements are described in the position paper \"DataJoint Elements: Data Workflows for Neurophysiology\" . Project Structure Management and Policies Design Principles Guidelines for Adoption Glossary Components The Resource provides the following main components: DataJoint the open-source framework for data pipelines and automated computational workflows + related documentation, tools, and utilities. DataJoint Elements a collection of curated modules for assembling workflows for the major modalities of neurophysiology experiments + related documentation, tools, and utilities. Elements Lab management Animal management Example workflow Experiment session Extracellular array electrophysiology Example workflow Calcium imaging Example workflow Miniscope imaging Example workflow The DataJoint framework DataJoint for Python DataJoint for MATLAB DataJoint Documentation DataJoint Tutorials Docker image for MySQL server configured for use with DataJoint Interfaces Pharus \u2014 a REST API for interacting with DataJoint databases DataJoint Labbook \u2014 a front-end web interface for viewing and entering data Online Training DataJoint interactive online tutorials","title":"About"},{"location":"index.html#datajoint-elements","text":"This resource (\"Resource\") provides an efficient approach for neuroscience labs to create and manage scientific data workflows : the complex multi-step methods for data collection, preparation, processing, analysis, and modeling that researchers must perform in the course of an experimental study. The work is derived from the developments in leading neuroscience projects and uses the DataJoint framework for defining, deploying, and sharing their data workflows. An overview of the principles of DataJoint workflows and the goals of DataJoint Elements are described in the position paper \"DataJoint Elements: Data Workflows for Neurophysiology\" .","title":"DataJoint Elements"},{"location":"index.html#project-structure","text":"Management and Policies Design Principles Guidelines for Adoption Glossary","title":"Project Structure"},{"location":"index.html#components","text":"The Resource provides the following main components: DataJoint the open-source framework for data pipelines and automated computational workflows + related documentation, tools, and utilities. DataJoint Elements a collection of curated modules for assembling workflows for the major modalities of neurophysiology experiments + related documentation, tools, and utilities.","title":"Components"},{"location":"index.html#elements","text":"Lab management Animal management Example workflow Experiment session Extracellular array electrophysiology Example workflow Calcium imaging Example workflow Miniscope imaging Example workflow","title":"Elements"},{"location":"index.html#the-datajoint-framework","text":"DataJoint for Python DataJoint for MATLAB DataJoint Documentation DataJoint Tutorials Docker image for MySQL server configured for use with DataJoint","title":"The DataJoint framework"},{"location":"index.html#interfaces","text":"Pharus \u2014 a REST API for interacting with DataJoint databases DataJoint Labbook \u2014 a front-end web interface for viewing and entering data","title":"Interfaces"},{"location":"index.html#online-training","text":"DataJoint interactive online tutorials","title":"Online Training"},{"location":"adopt.html","text":"Adopt You have several options for adopting DataJoint workflows for your own experiments. Adopt independently DataJoint Elements are designed for adoption by independent users with moderate software development skills, good understanding of DataJoint principles, and adequate IT expertise or support. If you have not yet used DataJoint, we recommend completing our online training tutorials or attending a workshop either online or in person. Tutorials: https://playground.datajoint.io https://tutorials.datajoint.io https://docs.datajoint.io Support from DataJoint NEURO Our team provides support to labs to adopt DataJoint workflows in their research. This includes: User training Developer training Data and computation hosting on your premises using your own cloud accounts fully managed cloud hosting by DataJoint Workflow execution configuration and automation optional fully managed service by DataJoint Neuro Interfaces for data entry, export and publishing During alpha and beta testing phases, these services may be subsidized by the grant funding for qualified research groups. To select the support service, apply here .","title":"Adopt"},{"location":"adopt.html#adopt","text":"You have several options for adopting DataJoint workflows for your own experiments.","title":"Adopt"},{"location":"adopt.html#adopt-independently","text":"DataJoint Elements are designed for adoption by independent users with moderate software development skills, good understanding of DataJoint principles, and adequate IT expertise or support. If you have not yet used DataJoint, we recommend completing our online training tutorials or attending a workshop either online or in person. Tutorials: https://playground.datajoint.io https://tutorials.datajoint.io https://docs.datajoint.io","title":"Adopt independently"},{"location":"adopt.html#support-from-datajoint-neuro","text":"Our team provides support to labs to adopt DataJoint workflows in their research. This includes: User training Developer training Data and computation hosting on your premises using your own cloud accounts fully managed cloud hosting by DataJoint Workflow execution configuration and automation optional fully managed service by DataJoint Neuro Interfaces for data entry, export and publishing During alpha and beta testing phases, these services may be subsidized by the grant funding for qualified research groups. To select the support service, apply here .","title":"Support from DataJoint NEURO"},{"location":"design-principles.html","text":"Design Principles The following conventions describe the Python implementation. Matlab conventions are similar and will be described separately. DataJoint Schemas DataJoint allows creating database schemas , which are namespaces for collections of related tables. The following commands declare a new schema and create the object named schema to reference the database schema. import datajoint as dj schema = dj.schema('<schema_name>') We follow the convention of having only one schema defined per Python module. Then such a module becomes a \"DataJoint schema\" comprising a python module with a corresponding database schema. The module's schema object is then used as the decorator for classes that define tables in the database. Elements An Element is a software package defining one or more DataJoint schemas serving a particular purpose. By convention, such packages are hosted in individual GitHub repositories. For example, Element element_calcium_imaging is hosted at https://github.com/datajoint/element-calcium-imaging , and contains two DataJoint schemas: scan and imaging . Deferred schemas A deferred schema is one in which the name of the database schema name is not specified. This module does not declare schema and tables upon import. Instead, they are declared by calling schema.activate('<schema_name>') after import. By convention, all modules corresponding to deferred schema must declare the function activate which in turn calls schema.activate . Thus Element modules begin with import datajoint as dj schema = dj.schema() def activate(schema_name): schema.activate(schema_name) However, many activate functions perform other work associated with activating the schema such as activating other schemas upstream. Linking Module To make the code more modular with fewer dependencies, Elements' modules do not import upstream schemas directly. Instead, all required classes and functions must be defined in a \"linking module\" and passed to the module's activate function. For instance, the element_calcium_imaging.scan module receives its required functions from the linking module passed into the module's activate function. See the corresponding workflow for an example of how the linking module is passed into the Element's module.","title":"Design Principles"},{"location":"design-principles.html#design-principles","text":"The following conventions describe the Python implementation. Matlab conventions are similar and will be described separately.","title":"Design Principles"},{"location":"design-principles.html#datajoint-schemas","text":"DataJoint allows creating database schemas , which are namespaces for collections of related tables. The following commands declare a new schema and create the object named schema to reference the database schema. import datajoint as dj schema = dj.schema('<schema_name>') We follow the convention of having only one schema defined per Python module. Then such a module becomes a \"DataJoint schema\" comprising a python module with a corresponding database schema. The module's schema object is then used as the decorator for classes that define tables in the database.","title":"DataJoint Schemas"},{"location":"design-principles.html#elements","text":"An Element is a software package defining one or more DataJoint schemas serving a particular purpose. By convention, such packages are hosted in individual GitHub repositories. For example, Element element_calcium_imaging is hosted at https://github.com/datajoint/element-calcium-imaging , and contains two DataJoint schemas: scan and imaging .","title":"Elements"},{"location":"design-principles.html#deferred-schemas","text":"A deferred schema is one in which the name of the database schema name is not specified. This module does not declare schema and tables upon import. Instead, they are declared by calling schema.activate('<schema_name>') after import. By convention, all modules corresponding to deferred schema must declare the function activate which in turn calls schema.activate . Thus Element modules begin with import datajoint as dj schema = dj.schema() def activate(schema_name): schema.activate(schema_name) However, many activate functions perform other work associated with activating the schema such as activating other schemas upstream.","title":"Deferred schemas"},{"location":"design-principles.html#linking-module","text":"To make the code more modular with fewer dependencies, Elements' modules do not import upstream schemas directly. Instead, all required classes and functions must be defined in a \"linking module\" and passed to the module's activate function. For instance, the element_calcium_imaging.scan module receives its required functions from the linking module passed into the module's activate function. See the corresponding workflow for an example of how the linking module is passed into the Element's module.","title":"Linking Module"},{"location":"glossary.html","text":"Glossary The following are some terms used in the Resource. DataJoint a software framework for database programming directly from matlab and python. Thanks to its support of automated computational dependencies, DataJoint serves as a workflow management system. DataJoint Workflow, Experiment Workflow, or simply Workflow a formal representation of the steps for executing an experiment from data collection to analysis. Also the software configured for performing these steps. A typical workflow is composed of tables with inter-dependencies and processes to compute and insert data into the tables. DataJoint Pipeline the data schemas and transformations underlying a DataJoint workflow. DataJoint allows defining code that specifies both the workflow and the data pipeline, and we have used the words \"pipeline\" and \"workflow\" almost interchangeably. DataJoint Schema a software module implementing a portion of an experiment workflow. Includes database table definitions, dependencies, and associated computations. DataJoint Elements software modules implementing portions of experiment workflows designed for ease of integration into diverse custom workflows. djHub our team's internal platform for delivering cloud-based infrastructure to support online training resources, validation studies, and collaborative projects.","title":"Glossary"},{"location":"glossary.html#glossary","text":"The following are some terms used in the Resource. DataJoint a software framework for database programming directly from matlab and python. Thanks to its support of automated computational dependencies, DataJoint serves as a workflow management system. DataJoint Workflow, Experiment Workflow, or simply Workflow a formal representation of the steps for executing an experiment from data collection to analysis. Also the software configured for performing these steps. A typical workflow is composed of tables with inter-dependencies and processes to compute and insert data into the tables. DataJoint Pipeline the data schemas and transformations underlying a DataJoint workflow. DataJoint allows defining code that specifies both the workflow and the data pipeline, and we have used the words \"pipeline\" and \"workflow\" almost interchangeably. DataJoint Schema a software module implementing a portion of an experiment workflow. Includes database table definitions, dependencies, and associated computations. DataJoint Elements software modules implementing portions of experiment workflows designed for ease of integration into diverse custom workflows. djHub our team's internal platform for delivering cloud-based infrastructure to support online training resources, validation studies, and collaborative projects.","title":"Glossary"},{"location":"support.html","text":"Support Thank you for your interest in engineering support for DataJoint Elements adoption! Please provide the following information. Contact Name: Contact Email: Institution: Lab: Describe data modalities (ephys, calcium imaging, etc.): Describe current workflow for data acquisition and analysis: Do you prefer to setup a database server yourself (on premises or cloud) or use DataJoint Neuro managed cloud hosting? When would you like to schedule the first test?","title":"Support"},{"location":"support.html#support","text":"Thank you for your interest in engineering support for DataJoint Elements adoption! Please provide the following information. Contact Name: Contact Email: Institution: Lab: Describe data modalities (ephys, calcium imaging, etc.): Describe current workflow for data acquisition and analysis: Do you prefer to setup a database server yourself (on premises or cloud) or use DataJoint Neuro managed cloud hosting? When would you like to schedule the first test?","title":"Support"},{"location":"management/contribute.html","text":"Contribution Guideline We have established the following guidelines for contributing to DataJoint and DataJoint Elements: https://docs.datajoint.io/python/community/02-Contribute.html General how-to questions: StackOverflow: datajoint tag Error questions: StackOverflow: datajoint tag Bug report: GitHub issue tracker New feature request: GitHub issue tracker Open-ended discussions: DataJoint Slack For additional support for our customers, we invite you to our DataJoint NEURO support page where you may file a ticket or start a forum topic for direct support.","title":"Contribution Guideline"},{"location":"management/contribute.html#contribution-guideline","text":"We have established the following guidelines for contributing to DataJoint and DataJoint Elements: https://docs.datajoint.io/python/community/02-Contribute.html General how-to questions: StackOverflow: datajoint tag Error questions: StackOverflow: datajoint tag Bug report: GitHub issue tracker New feature request: GitHub issue tracker Open-ended discussions: DataJoint Slack For additional support for our customers, we invite you to our DataJoint NEURO support page where you may file a ticket or start a forum topic for direct support.","title":"Contribution Guideline"},{"location":"management/governance.html","text":"Project Governance Funding This Resource is supported by the National Institute Of Neurological Disorders And Stroke of the National Institutes of Health under Award Number U24NS116470. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. Scientific Steering Group The project oversight and guidance is provided by the Scientific Steering Group comprising Mackenzie Mathis (EPFL) John Cunningham (Columbia U) Carlos Brody (Princeton U) Karel Svoboda (Janelia) Nick Steinmetz (U of Washington) Loren Frank (UCSF)","title":"Project Governance"},{"location":"management/governance.html#project-governance","text":"","title":"Project Governance"},{"location":"management/governance.html#funding","text":"This Resource is supported by the National Institute Of Neurological Disorders And Stroke of the National Institutes of Health under Award Number U24NS116470. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.","title":"Funding"},{"location":"management/governance.html#scientific-steering-group","text":"The project oversight and guidance is provided by the Scientific Steering Group comprising Mackenzie Mathis (EPFL) John Cunningham (Columbia U) Carlos Brody (Princeton U) Karel Svoboda (Janelia) Nick Steinmetz (U of Washington) Loren Frank (UCSF)","title":"Scientific Steering Group"},{"location":"management/licenses.html","text":"Licenses and User Agreements All components of provided by the Resource are distributed under permissive open-source licenses. These licenses are included in the respective repositories and are summarized below: DataJoint for Python: LGPLv3 License DataJoint for MATLAB: MIT License Pharus: MIT License DataJoint LabBook: MIT License DataJoint Elements and Workflows: MIT License","title":"Licenses and User Agreements"},{"location":"management/licenses.html#licenses-and-user-agreements","text":"All components of provided by the Resource are distributed under permissive open-source licenses. These licenses are included in the respective repositories and are summarized below: DataJoint for Python: LGPLv3 License DataJoint for MATLAB: MIT License Pharus: MIT License DataJoint LabBook: MIT License DataJoint Elements and Workflows: MIT License","title":"Licenses and User Agreements"},{"location":"management/outreach.html","text":"Outreach Plan Broad engagement with the neuroscience community is necessary for the optimization, integration, and adoption of the Resource components. We conduct five types of outreach activities that require different approaches: 1. Precursor Projects Our Selection Process requires a \"Precursor Project\" for any new experiment modality to be included in DataJoint Elements. A precursor project is a project that develops a DataJoint pipeline for its own experiments either independently or in collaboration with our team. We reach out to teams who develop DataJoint pipelines for new experiment paradigms and modalities to identify essential design motifs, analysis tools, and related tools and interfaces. We interview the core team to learn about their collaborative culture, practices, and procedures. We jointly review their open-source code and their plans for disseminating it. In many cases, our team already collaborates with such teams through our other projects and we have a good understanding of their process. As we develop a new Element to support the new modality, we remain in contact with the team to include their contribution, solicit feedback, and evaluate design tradeoffs, When the new Element is released, a full attribution is given to the Precursor Project. Rationale: The Resource does not aim to develop fundamentally new solutions for neurophysiology data acquisition and analysis. Rather it aims to systematize and disseminate existing open-source tools proven in leading research projects. 2. Tool Developers DataJoint pipelines rely on analysis tools, atlases, data standards, archives and catalogs, and other neuroinformatics resources developed and maintained by the broader scientific community. To ensure sustainability of the Resource, we reach out to the tool developer to establish joint sustainability roadmaps. 3. Validation Sites Before new Resource components are released as part of this Resource, our team engages other research groups to evaluate these components in their projects in realistic scenarios experiments. During the Alpha testing period, the validation sites set up the workflow by either integrating Resource components with their existing workflow or by combining only the Resource components with the help of our team. We evaluate the feedback and upgrade our Resource components accordingly during this process. 4. Dissemination We conduct activities to disseminate Resource components for adoption in diverse neuroscience labs. These activities include A central website for the Resource. Presently, the site https://elements.datajoint.org serves this purpose. Conference talks, presentations, and workshops Publications in peer-reviewed journals White papers posted on internet resources and websites On-site workshops by invitation Remote workshops and webinars Online interactive tutorials 5. Census In order to measure the effectiveness of the Resource, we conduct several activities to estimate the adoption and use of the Resource components: A citation mechanims for individual components of the Resource Collect summary statistics of the number of downloads and repository forking. A register for self-reporting for component adoption and use.","title":"Outreach Plan"},{"location":"management/outreach.html#outreach-plan","text":"Broad engagement with the neuroscience community is necessary for the optimization, integration, and adoption of the Resource components. We conduct five types of outreach activities that require different approaches:","title":"Outreach Plan"},{"location":"management/outreach.html#1-precursor-projects","text":"Our Selection Process requires a \"Precursor Project\" for any new experiment modality to be included in DataJoint Elements. A precursor project is a project that develops a DataJoint pipeline for its own experiments either independently or in collaboration with our team. We reach out to teams who develop DataJoint pipelines for new experiment paradigms and modalities to identify essential design motifs, analysis tools, and related tools and interfaces. We interview the core team to learn about their collaborative culture, practices, and procedures. We jointly review their open-source code and their plans for disseminating it. In many cases, our team already collaborates with such teams through our other projects and we have a good understanding of their process. As we develop a new Element to support the new modality, we remain in contact with the team to include their contribution, solicit feedback, and evaluate design tradeoffs, When the new Element is released, a full attribution is given to the Precursor Project. Rationale: The Resource does not aim to develop fundamentally new solutions for neurophysiology data acquisition and analysis. Rather it aims to systematize and disseminate existing open-source tools proven in leading research projects.","title":"1. Precursor Projects"},{"location":"management/outreach.html#2-tool-developers","text":"DataJoint pipelines rely on analysis tools, atlases, data standards, archives and catalogs, and other neuroinformatics resources developed and maintained by the broader scientific community. To ensure sustainability of the Resource, we reach out to the tool developer to establish joint sustainability roadmaps.","title":"2. Tool Developers"},{"location":"management/outreach.html#3-validation-sites","text":"Before new Resource components are released as part of this Resource, our team engages other research groups to evaluate these components in their projects in realistic scenarios experiments. During the Alpha testing period, the validation sites set up the workflow by either integrating Resource components with their existing workflow or by combining only the Resource components with the help of our team. We evaluate the feedback and upgrade our Resource components accordingly during this process.","title":"3. Validation Sites"},{"location":"management/outreach.html#4-dissemination","text":"We conduct activities to disseminate Resource components for adoption in diverse neuroscience labs. These activities include A central website for the Resource. Presently, the site https://elements.datajoint.org serves this purpose. Conference talks, presentations, and workshops Publications in peer-reviewed journals White papers posted on internet resources and websites On-site workshops by invitation Remote workshops and webinars Online interactive tutorials","title":"4. Dissemination"},{"location":"management/outreach.html#5-census","text":"In order to measure the effectiveness of the Resource, we conduct several activities to estimate the adoption and use of the Resource components: A citation mechanims for individual components of the Resource Collect summary statistics of the number of downloads and repository forking. A register for self-reporting for component adoption and use.","title":"5. Census"},{"location":"management/plan.html","text":"Management Plan DataJoint Elements has established a Resource Management Plan to select projects for development, to assure quality, and to disseminate its output as summarized in the figure below: The following sections provide detailed information. Team Project Governance Project Selection Process Quality Assurance Contribution Guideline Outreach Plan Licenses and User Agreements","title":"Management Plan"},{"location":"management/plan.html#management-plan","text":"DataJoint Elements has established a Resource Management Plan to select projects for development, to assure quality, and to disseminate its output as summarized in the figure below: The following sections provide detailed information. Team Project Governance Project Selection Process Quality Assurance Contribution Guideline Outreach Plan Licenses and User Agreements","title":"Management Plan"},{"location":"management/quality-assurance.html","text":"Quality Assurance DataJoint and DataJoint Elements serve as a framework and starting points for numerous new projects, setting the standard of quality for data architecture and software design. To ensure higher quality, the following policies have been adopted into the software development lifecycle (SDLC). Coding Standards When writting code, the following principles should be observed. Style : Code shall be written for clear readability. Uniform and clear naming conventions, module structure, and formatting requirements shall be established across all components of the project. Python's PEP8 standard offers clear guidance to this regard which can similarly be applied to all languages. Maintenance Overhead : Code base size should be noted to prevent large, unnecessarily complex solutions from being introduced. The idea is that the larger the code base, the more there is to review and maintain. Therefore, we should aim to find a compromise where we can keep the code base from becoming too large without adding convoluted complexity. Performance : Performance drawbacks should be avoided, controlled, or, at least, be properly monitored and justified. For instance: memory management, garbage collection, disk reads/writes, and processing overhead should be regarded to ensure that an efficient solution is achieved. Automated Testing All components and their revisions must include appropriate automated software testing to be considered for release. The core framework must undergo thorough performance evaluation and comprehensive integration testing. Generally, this includes tests related to: Syntax : Verify that the code base does not contain any syntax errors and will run or compile successfully. Unit & Integration : Verify that low-level, method-specific tests (unit tests) and any tests related coordinated interface between methods (integration tests) pass successfully. Typically, when bugs are patched or features are introduced, unit and integration tests are added to ensure that the use-case intended to be satisfied is accounted for. This helps us prevent any regression in functionality. Style : Verify that the code base adheres to style guides for optimal readability. Code Coverage : Verify that the code base has similar or better code coverage than the last run. Code Reviews When introducing new code to the code base, the following will be required for acceptance by DataJoint core team into the main code repository. Independence : Proposed changes should not directly alter the code base in the review process. New changes should be applied separately on a copy of the code base and proposed for review by the DataJoint core team. For example, apply changes on a GitHub fork and open a pull request targeting the main branch once ready for review. Etiquette : An author who has requested for a code for review should not accept and merge their own code to the code base. A reviewer should not commit any suggestions directly to the authors proposed changes but rather should allow the author to review. Coding Standards : Ensure the above coding standards are respected. Summary : A description should be included that summarizes and highlights the notable changes that are being proposed. Issue Reference : Any bugs or feature requests that have been filed in the issue tracker that would be resolved by acceptance should be properly linked and referenced. Satisfy Automated Tests : All automated tests associated with the project will be verified to be successful prior to acceptance. Documentation : Documentation should be included to reflect any new feature or behavior introduced. Release Notes : Include necessary updates to the release notes or change log to capture a summary of the patched bugs and new feature introduction. Proper linking should be maintained to associated tickets in issue tracker and reviews. Alpha Release Process For the workflows and their revisions, the initial development and internal testing, the code will be released in Alpha . During this phase, we will engage external research teams to test and validate the complete workflows in real-life experiments with our team's engineering support. During this phase, significant design changes may be performed and not all features may be completely developed. However, several features should be usable and suitable for testing and validation. Criteria to participate as validation sites The participating lab/group has an existing DataJoint pipeline for the Element(s) to be connected to The DataJoint pipeline code base is hosted as a Github repository (either public or private) The participating lab/group has existing datasets in the format supported by the Element(s) for the purpose of this validation The participating lab/group has a dedicated point of contact person to work closely with DataJoint NEURO engineering team for this validation effort Criteria for a successful validation Able to connect the Element(s) to existing pipeline and all schemas/tables declared without errors Successful ingestion of data for at least 2 experimental sessions Inspection/verification that the data are ingested correctly by the participating lab/group - e.g. manual inspection of ingested traces, plotting of spatial footprints, spike trains, etc. to confirm correctness of ingested data Beta Release Process After the initial validation phase, we make the workflows available to the general public with a warning of Beta status and that the released code may be subject to errors and changes. During this phase, feature development should be complete with a focus on collecting user feedback to make design improvements and bug fixes. Official Release Process After gaining confidence of user satisfaction by resolving concerns raised in Beta , the workflows are declared officially released and announced to the community. Maintenance Support Lifecycle Revision of the workflows will be released with a version specification that clearly identifies whether in Alpha , Beta , or Official release status. Quality assurance process will be followed for all iterations and new designs. If the updates require changes in the design of the database schema or formats, a process for data migration will be provided. User Feedback & Issue Tracking All components will be organized in GitHub repositories with guidelines for contribution, feedback, and issue submission to the issue tracker. For more information on the general policy around issue filing, tracking, and escalation, see the DataJoint Open-Source Contribute policy. Typically issues will be prioritized based on their criticality and impact. If new feature requirements become apparent, this may trigger the creation of a separate workflow or a major revision of an existing workflow.","title":"Quality Assurance"},{"location":"management/quality-assurance.html#quality-assurance","text":"DataJoint and DataJoint Elements serve as a framework and starting points for numerous new projects, setting the standard of quality for data architecture and software design. To ensure higher quality, the following policies have been adopted into the software development lifecycle (SDLC).","title":"Quality Assurance"},{"location":"management/quality-assurance.html#coding-standards","text":"When writting code, the following principles should be observed. Style : Code shall be written for clear readability. Uniform and clear naming conventions, module structure, and formatting requirements shall be established across all components of the project. Python's PEP8 standard offers clear guidance to this regard which can similarly be applied to all languages. Maintenance Overhead : Code base size should be noted to prevent large, unnecessarily complex solutions from being introduced. The idea is that the larger the code base, the more there is to review and maintain. Therefore, we should aim to find a compromise where we can keep the code base from becoming too large without adding convoluted complexity. Performance : Performance drawbacks should be avoided, controlled, or, at least, be properly monitored and justified. For instance: memory management, garbage collection, disk reads/writes, and processing overhead should be regarded to ensure that an efficient solution is achieved.","title":"Coding Standards"},{"location":"management/quality-assurance.html#automated-testing","text":"All components and their revisions must include appropriate automated software testing to be considered for release. The core framework must undergo thorough performance evaluation and comprehensive integration testing. Generally, this includes tests related to: Syntax : Verify that the code base does not contain any syntax errors and will run or compile successfully. Unit & Integration : Verify that low-level, method-specific tests (unit tests) and any tests related coordinated interface between methods (integration tests) pass successfully. Typically, when bugs are patched or features are introduced, unit and integration tests are added to ensure that the use-case intended to be satisfied is accounted for. This helps us prevent any regression in functionality. Style : Verify that the code base adheres to style guides for optimal readability. Code Coverage : Verify that the code base has similar or better code coverage than the last run.","title":"Automated Testing"},{"location":"management/quality-assurance.html#code-reviews","text":"When introducing new code to the code base, the following will be required for acceptance by DataJoint core team into the main code repository. Independence : Proposed changes should not directly alter the code base in the review process. New changes should be applied separately on a copy of the code base and proposed for review by the DataJoint core team. For example, apply changes on a GitHub fork and open a pull request targeting the main branch once ready for review. Etiquette : An author who has requested for a code for review should not accept and merge their own code to the code base. A reviewer should not commit any suggestions directly to the authors proposed changes but rather should allow the author to review. Coding Standards : Ensure the above coding standards are respected. Summary : A description should be included that summarizes and highlights the notable changes that are being proposed. Issue Reference : Any bugs or feature requests that have been filed in the issue tracker that would be resolved by acceptance should be properly linked and referenced. Satisfy Automated Tests : All automated tests associated with the project will be verified to be successful prior to acceptance. Documentation : Documentation should be included to reflect any new feature or behavior introduced. Release Notes : Include necessary updates to the release notes or change log to capture a summary of the patched bugs and new feature introduction. Proper linking should be maintained to associated tickets in issue tracker and reviews.","title":"Code Reviews"},{"location":"management/quality-assurance.html#alpha-release-process","text":"For the workflows and their revisions, the initial development and internal testing, the code will be released in Alpha . During this phase, we will engage external research teams to test and validate the complete workflows in real-life experiments with our team's engineering support. During this phase, significant design changes may be performed and not all features may be completely developed. However, several features should be usable and suitable for testing and validation.","title":"Alpha Release Process"},{"location":"management/quality-assurance.html#criteria-to-participate-as-validation-sites","text":"The participating lab/group has an existing DataJoint pipeline for the Element(s) to be connected to The DataJoint pipeline code base is hosted as a Github repository (either public or private) The participating lab/group has existing datasets in the format supported by the Element(s) for the purpose of this validation The participating lab/group has a dedicated point of contact person to work closely with DataJoint NEURO engineering team for this validation effort","title":"Criteria to participate as validation sites"},{"location":"management/quality-assurance.html#criteria-for-a-successful-validation","text":"Able to connect the Element(s) to existing pipeline and all schemas/tables declared without errors Successful ingestion of data for at least 2 experimental sessions Inspection/verification that the data are ingested correctly by the participating lab/group - e.g. manual inspection of ingested traces, plotting of spatial footprints, spike trains, etc. to confirm correctness of ingested data","title":"Criteria for a successful validation"},{"location":"management/quality-assurance.html#beta-release-process","text":"After the initial validation phase, we make the workflows available to the general public with a warning of Beta status and that the released code may be subject to errors and changes. During this phase, feature development should be complete with a focus on collecting user feedback to make design improvements and bug fixes.","title":"Beta Release Process"},{"location":"management/quality-assurance.html#official-release-process","text":"After gaining confidence of user satisfaction by resolving concerns raised in Beta , the workflows are declared officially released and announced to the community.","title":"Official Release Process"},{"location":"management/quality-assurance.html#maintenance-support-lifecycle","text":"Revision of the workflows will be released with a version specification that clearly identifies whether in Alpha , Beta , or Official release status. Quality assurance process will be followed for all iterations and new designs. If the updates require changes in the design of the database schema or formats, a process for data migration will be provided.","title":"Maintenance Support Lifecycle"},{"location":"management/quality-assurance.html#user-feedback-issue-tracking","text":"All components will be organized in GitHub repositories with guidelines for contribution, feedback, and issue submission to the issue tracker. For more information on the general policy around issue filing, tracking, and escalation, see the DataJoint Open-Source Contribute policy. Typically issues will be prioritized based on their criticality and impact. If new feature requirements become apparent, this may trigger the creation of a separate workflow or a major revision of an existing workflow.","title":"User Feedback &amp; Issue Tracking"},{"location":"management/selection.html","text":"Project Selection Process The project milestones are set annually by the team under the stewardship of the NIH programmatic staff and with the guidance of the project's Scientific Steering Group We have adopted the following general criteria for selecting and accepting new projects to be included in the Resource. Open Precursor Project(s) At least one open-source DataJoint-based precursor project must exist for any new experiment modality to be accepted for support as part of the Resource. The precursor project team must be open to interviews to describe in detail their process for the experiment workflow, tools, and interfaces. The precursor projects must provide sample data for testing during development and for tutorials. The precursor projects will be acknowledged in the development of the component. Rationale: This Resource does not aim to develop fundamentally new solutions for neurophysiology data acquisition and analysis. Rather it seeks to systematize and disseminate existing open-source tools proven in leading research projects. Impact New components proposed for support in the project must be shown to be in demand by a substantial population or research groups, on the order of 100+ labs globally. Validation Roadmap Several new research groups must express interest in becoming the validation sites during the alpha and beta release phases of the project. Our team will work closely with the validation sites to test the components in their workflow and collect feedback during the process. Sustainability For all third-party tools or resources included in the proposed component, their long-term maintenance roadmap must be established. When possible, we will contact the developer team and work with them to establish a sustainability roadmap. If no such roadmap can be established, alternative tools and resources must be identified as replacement.","title":"Project Selection Process"},{"location":"management/selection.html#project-selection-process","text":"The project milestones are set annually by the team under the stewardship of the NIH programmatic staff and with the guidance of the project's Scientific Steering Group We have adopted the following general criteria for selecting and accepting new projects to be included in the Resource. Open Precursor Project(s) At least one open-source DataJoint-based precursor project must exist for any new experiment modality to be accepted for support as part of the Resource. The precursor project team must be open to interviews to describe in detail their process for the experiment workflow, tools, and interfaces. The precursor projects must provide sample data for testing during development and for tutorials. The precursor projects will be acknowledged in the development of the component. Rationale: This Resource does not aim to develop fundamentally new solutions for neurophysiology data acquisition and analysis. Rather it seeks to systematize and disseminate existing open-source tools proven in leading research projects. Impact New components proposed for support in the project must be shown to be in demand by a substantial population or research groups, on the order of 100+ labs globally. Validation Roadmap Several new research groups must express interest in becoming the validation sites during the alpha and beta release phases of the project. Our team will work closely with the validation sites to test the components in their workflow and collect feedback during the process. Sustainability For all third-party tools or resources included in the proposed component, their long-term maintenance roadmap must be established. When possible, we will contact the developer team and work with them to establish a sustainability roadmap. If no such roadmap can be established, alternative tools and resources must be identified as replacement.","title":"Project Selection Process"},{"location":"management/team.html","text":"Team The project is performed by DataJoint Neuro with Dimitri Yatsenko as Principal Investigator. Scientists Dimitri Yatsenko - PI Thinh Nguyen - Data Scientist Shan Shen - Data Scientist Kabilar Gunalan - Data Scientist Engineers Raphael Guzman - Software Engineering Christopher Turner - Data Systems Engineering Maho Sasaki - Frontend Developer Daniel Sitonic - Backend Developer, Software Engineer Past contributors Edgar Y. Walker - System architect, Data Scientist, Project Manager (from project start to Jan, 2021) Andreas S. Tolias - grant proposal contributor Jacob Reimer - grant proposal contributor The first-person pronouns \"we\" and \"our\" in these documents refer to the Performer Team. External contributors The principal components of the Rescource are developed and distributed as open-source projects and external contributions are welcome. We have adopted the following Contribution Guide for DataJoint, DataJoint Elements, and related open-source tools: https://docs.datajoint.io/python/community/02-Contribute.html","title":"Team"},{"location":"management/team.html#team","text":"The project is performed by DataJoint Neuro with Dimitri Yatsenko as Principal Investigator.","title":"Team"},{"location":"management/team.html#scientists","text":"Dimitri Yatsenko - PI Thinh Nguyen - Data Scientist Shan Shen - Data Scientist Kabilar Gunalan - Data Scientist","title":"Scientists"},{"location":"management/team.html#engineers","text":"Raphael Guzman - Software Engineering Christopher Turner - Data Systems Engineering Maho Sasaki - Frontend Developer Daniel Sitonic - Backend Developer, Software Engineer","title":"Engineers"},{"location":"management/team.html#past-contributors","text":"Edgar Y. Walker - System architect, Data Scientist, Project Manager (from project start to Jan, 2021) Andreas S. Tolias - grant proposal contributor Jacob Reimer - grant proposal contributor The first-person pronouns \"we\" and \"our\" in these documents refer to the Performer Team.","title":"Past contributors"},{"location":"management/team.html#external-contributors","text":"The principal components of the Rescource are developed and distributed as open-source projects and external contributions are welcome. We have adopted the following Contribution Guide for DataJoint, DataJoint Elements, and related open-source tools: https://docs.datajoint.io/python/community/02-Contribute.html","title":"External contributors"}]}