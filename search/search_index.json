{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DataJoint Elements for Neurophysiology DataJoint Elements provides an efficient approach for neuroscience labs to create and manage scientific data workflows : the complex multi-step methods for data collection, preparation, processing, analysis, and modeling that researchers must perform in the course of an experimental study. The work is derived from the developments in leading neuroscience projects and uses the DataJoint framework for defining, deploying, and sharing their data workflows. DataJoint the open-source framework for data pipelines and automated computational workflows + related documentation, tools, and utilities. DataJoint Elements a collection of curated modules for assembling workflows for the major modalities of neurophysiology experiments + related documentation, tools, and utilities. An overview of the principles of DataJoint workflows and the goals of DataJoint Elements are described in the position paper \"DataJoint Elements: Data Workflows for Neurophysiology\" . Project Structure Management and Policies Design Principles Guidelines for Adoption Glossary DataJoint Elements DataJoint Element Example workflow Backgound, Features, and Status Lab management See Session Link Animal management See Session Link Experiment Session management Lab, subject and session management Link Extracellular array electrophysiology Neuropixels Link Calcium imaging Multiphoton laser scanning Link Miniscope imaging UCLA Miniscope Link DataJoint Elements -- in development DataJoint Element Example workflow Description DeepLabCut Workflow for pose tracking Background, features, and status Event- and trial-based experiments Workflow for Neuropixels Workflow for multiphoton laser scanning Background, features, and status Electrode localization for Neuropixels Workflow for Neuropixels Background, features, and status Facemap In development Background, features, and status DataJoint framework DataJoint for Python DataJoint for MATLAB DataJoint Documentation DataJoint Tutorials Docker image for MySQL server configured for use with DataJoint DataJoint Interfaces Pharus \u2014 a REST API for interacting with DataJoint databases DataJoint LabBook \u2014 a front-end web interface for viewing and entering data DataJoint SciViz \u2014 a low-code framework for building websites for interactive data visualizaion. DataJoint Online Training DataJoint CodeBook - interactive online tutorials Feedback DataJoint and DataJoint Elements are supported by NIH grant U24 NS116470 for disseminating open-source software for neuroscience research. Your feedback is essential for continued funding. Your feedback also helps shape the technology development roadmap for the DataJoint ecosystem. Please tell us about your projects by filling out the DataJoint Census . Citation If your work uses DataJoint or DataJoint Elements, please cite the following: DataJoint Yatsenko D, Reimer J, Ecker AS, Walker EY, Sinz F, Berens P, Hoenselaar A, Cotton RJ, Siapas AS, Tolias AS. DataJoint: managing big scientific data using MATLAB or Python. bioRxiv. 2015 Jan 1:031658. DataJoint Elements Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1.","title":"DataJoint Elements for Neurophysiology"},{"location":"#datajoint-elements-for-neurophysiology","text":"DataJoint Elements provides an efficient approach for neuroscience labs to create and manage scientific data workflows : the complex multi-step methods for data collection, preparation, processing, analysis, and modeling that researchers must perform in the course of an experimental study. The work is derived from the developments in leading neuroscience projects and uses the DataJoint framework for defining, deploying, and sharing their data workflows. DataJoint the open-source framework for data pipelines and automated computational workflows + related documentation, tools, and utilities. DataJoint Elements a collection of curated modules for assembling workflows for the major modalities of neurophysiology experiments + related documentation, tools, and utilities. An overview of the principles of DataJoint workflows and the goals of DataJoint Elements are described in the position paper \"DataJoint Elements: Data Workflows for Neurophysiology\" .","title":"DataJoint Elements for Neurophysiology"},{"location":"#project-structure","text":"Management and Policies Design Principles Guidelines for Adoption Glossary","title":"Project Structure"},{"location":"#datajoint-elements","text":"DataJoint Element Example workflow Backgound, Features, and Status Lab management See Session Link Animal management See Session Link Experiment Session management Lab, subject and session management Link Extracellular array electrophysiology Neuropixels Link Calcium imaging Multiphoton laser scanning Link Miniscope imaging UCLA Miniscope Link","title":"DataJoint Elements"},{"location":"#datajoint-elements-in-development","text":"DataJoint Element Example workflow Description DeepLabCut Workflow for pose tracking Background, features, and status Event- and trial-based experiments Workflow for Neuropixels Workflow for multiphoton laser scanning Background, features, and status Electrode localization for Neuropixels Workflow for Neuropixels Background, features, and status Facemap In development Background, features, and status","title":"DataJoint Elements -- in development"},{"location":"#datajoint-framework","text":"DataJoint for Python DataJoint for MATLAB DataJoint Documentation DataJoint Tutorials Docker image for MySQL server configured for use with DataJoint","title":"DataJoint framework"},{"location":"#datajoint-interfaces","text":"Pharus \u2014 a REST API for interacting with DataJoint databases DataJoint LabBook \u2014 a front-end web interface for viewing and entering data DataJoint SciViz \u2014 a low-code framework for building websites for interactive data visualizaion.","title":"DataJoint Interfaces"},{"location":"#datajoint-online-training","text":"DataJoint CodeBook - interactive online tutorials","title":"DataJoint Online Training"},{"location":"#feedback","text":"DataJoint and DataJoint Elements are supported by NIH grant U24 NS116470 for disseminating open-source software for neuroscience research. Your feedback is essential for continued funding. Your feedback also helps shape the technology development roadmap for the DataJoint ecosystem. Please tell us about your projects by filling out the DataJoint Census .","title":"Feedback"},{"location":"#citation","text":"If your work uses DataJoint or DataJoint Elements, please cite the following: DataJoint Yatsenko D, Reimer J, Ecker AS, Walker EY, Sinz F, Berens P, Hoenselaar A, Cotton RJ, Siapas AS, Tolias AS. DataJoint: managing big scientific data using MATLAB or Python. bioRxiv. 2015 Jan 1:031658. DataJoint Elements Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1.","title":"Citation"},{"location":"projects/","text":"Projects DataJoint was originally developed by working systems neuroscientists at Baylor College of Medicine to meet the needs of their own research. Below is a partial list of known teams who use DataJoint. Please let us know if you would like to add another group or make other corrections by submitting a GitHub issue . Multi-lab collaboratives International Brain Lab ( GitHub ) Mesoscale Activity Project MICrONS Sainsbury Wellcome Centre Aeon U19 Projects NYU Osmonauts Harvard DOPE Columbia MoC3 Princeton BRAIN CoGS ( GitHub ) Rochester-NYU-Harvard Neural basis of causal inference Individual Labs Allen Institute Mindscope Program Karel Svoboda Lab ( Previously at Janelia Research Campus ) Arizona State University Rick Gerkin Lab Baylor College of Medicine Nuo Li Lab Matthew McGinley Lab Paul Pfaffinger Lab Jacob Reimer Lab Andreas Tolias Lab Boston University Jerry Chen Lab Benjamin Scott Lab California Institute of Technology Siapas Lab Cold Spring Harbor Laboratory Engel Lab Columbia University's Zuckerman Institute Mark Churchland Lab Rui Costa Lab EPFL Mackenzie Mathis Lab FORTH Emmanouil Froudarakis Lab ( GitHub ) Harvard Medical School Datta Lab Jan Drugowitsch Lab Harvey Lab Sabatini Lab Stelios Smirnakis Lab Indiana University Lu Lab Johns Hopkins University Applied Physics Lab ( GitHub ) Kavli Institute for Systems Neuroscience Moser Group Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen Busse Lab Katzner Lab MIT Fan Wang Lab New York University Dora Angelaki Lab Northwestern University James Cotton Lab ( GitHub ) Lucas Pinto Lab Princeton University Carlos Brody Lab David Tank Lab Ilana Witten Lab Jonathan Pillow Lab Seung Lab Sainsbury Wellcome Centre Tiago Branco Lab ( GitHub ) Stanford University Karl Deisseroth Lab Shaul Druckmann Lab Tel-Aviv University Pablo Blinder Lab ( GitHub ) University of Bonn Tobias Rose Lab University of California, Los Angeles Anne Churchland Lab University of California, San Diego David Kleinfeld Lab ( GitHub ) University of California, San Francisco Loren Frank Lab University of Oregon Santiago Jaramillo Lab ( GitHub ) Michael Wehr Lab ( GitHub ) University of Rochester Greg DeAngelis Lab Ralf Haefner Lab University of Washington Edgar Y. Walker Lab Tuthill Lab Wilhelm Schickard Institute for Computer Science Sinz Lab Berens Lab Euler Lab Bethge Lab \u2026 and more labs","title":"Projects"},{"location":"projects/#projects","text":"DataJoint was originally developed by working systems neuroscientists at Baylor College of Medicine to meet the needs of their own research. Below is a partial list of known teams who use DataJoint. Please let us know if you would like to add another group or make other corrections by submitting a GitHub issue .","title":"Projects"},{"location":"publications/","text":"Publications The following publications relied on DataJoint open-source software for data analysis. If you would like make additions or corrections, please submit an issue in the datajoint-elements repository . If your work uses DataJoint or DataJoint Elements, please cite the following: DataJoint Yatsenko D, Reimer J, Ecker AS, Walker EY, Sinz F, Berens P, Hoenselaar A, Cotton RJ, Siapas AS, Tolias AS. DataJoint: managing big scientific data using MATLAB or Python . bioRxiv. 2015 Jan 1:031658. Yatsenko D, Walker EY, Tolias AS. DataJoint: a simpler relational data model . arXiv:1807.11104. 2018 Jul 29. DataJoint Elements Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology . bioRxiv. 2021 Jan 1. 2022 Cotton, R. J. (2022). PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research . arXiv . 2203.08792. Cotton, R. J., McClerklin, E., Cimorelli, A., & Patel, A. (2022). Spatiotemporal characterization of gait from monocular videos with transformers . Cotton, R. J., McClerklin, E., Cimorelli, A., Patel, A., & Karakostas, T. (2022). Transforming Gait: Video-Based Spatiotemporal Gait Analysis . arXiv . 2203.09371. Jaffe, A. (2022). Optical investigation of microcircuit computations in mouse primary visual cortex . Doctoral dissertation Obenhaus, H.A., Zong, W., Jacobsen, R.I., Rose, T., Donato, F., Chen, L., Cheng, H., Bonhoeffer, T., Moser, M.B. & Moser, E.I. (2022). Functional network topography of the medial entorhinal cortex . Proceedings of the National Academy of Sciences , 119 (7). Ustyuzhaninov, I., Burg, M.F., Cadena, S.A., Fu, J., Muhammad, T., Ponder, K., Froudarakis, E., Ding, Z., Bethge, M., Tolias, A. & Ecker, A.S. Digital twin reveals combinatorial code of non-linear computations in the mouse primary visual cortex . bioRxiv . 2021 Bae, J. A., Baptiste, M., Bodor, A. L., Brittain, D., Buchanan, J., Bumbarger, D. J., Castro, M. A., Celii, B., Cobos, E., Collman, F.others. (2021). Functional connectomics spanning multiple areas of mouse visual cortex. bioRxiv . Born, G., Schneider-Soupiadis, F. A., Erisken, S., Vaiceliunaite, A., Lao, C. L., Mobarhan, M. H., Spacek, M. A., Einevoll, G. T., & Busse, L. (2021). Corticothalamic feedback sculpts visual spatial integration in mouse thalamus. Nature Neuroscience , 24 (12), 1711\u20131720. Burg, M. F., Cadena, S. A., Denfield, G. H., Walker, E. Y., Tolias, A. S., Bethge, M., & Ecker, A. S. (2021). Learning divisive normalization in primary visual cortex. PLOS Computational Biology , 17 (6), e1009028. Claudi, F., Campagner, D., & Branco, T. (2021). Innate heuristics and fast learning support escape route selection in mice. bioRxiv . Cohrs, K.-H. (2021). Investigation of feedback mechanisms in visual cortex using deep learning models [Master\u2019s thesis]. University of G\u00f6ttingen. Finkelstein, A., Fontolan, L., Economo, M. N., Li, N., Romani, S., & Svoboda, K. (2021). Attractor dynamics gate cortical information flow during decision-making . Nature Neuroscience . 24 (6), 843-850. Franke, K., Willeke, K. F., Ponder, K., Galdamez, M., Muhammad, T., Patel, S., Froudarakis, E., Reimer, J., Sinz, F., & Tolias, A. (2021). Behavioral state tunes mouse vision to ethological features through pupil dilation. bioRxiv . Jacobsen, R. I., Nair, R. R., Obenhaus, H. A., Donato, F., Slettmoen, T., Moser, M.-B., & Moser, E. I. (2021). All-viral tracing of monosynaptic inputs to single birthdate-defined neurons in the intact brain. bioRxiv . Laboratory, T. I. B., Aguillon-Rodriguez, V., Angelaki, D., Bayer, H., Bonacchi, N., Carandini, M., Cazettes, F., Chapuis, G., Churchland, A. K., Dan, Y.others. (2021). Standardized and reproducible measurement of decision-making in mice. eLife , 10 . Strauss, S., Korympidou, M. M., Ran, Y., Franke, K., Schubert, T., Baden, T., Berens, P., Euler, T., & Vlasits, A. L. (2021). Center-surround interactions underlie bipolar cell motion sensing in the mouse retina. bioRxiv . Subramaniyan, M., Manivannan, S., Chelur, V., Tsetsenis, T., Jiang, E., & Dani, J. A. (2021). Fear conditioning potentiates the hippocampal CA1 commissural pathway in vivo and increases awake phase sleep. Hippocampus , 31 (10), 1154\u20131175. Urai, A. E., Aguillon-Rodriguez, V., Laranjeira, I. C., Cazettes, F., Laboratory, T. I. B., Mainen, Z. F., & Churchland, A. K. (2021). Citric acid water as an alternative to water restriction for high-yield mouse behavior. Eneuro , 8 (1). Wal, A., Klein, F. J., Born, G., Busse, L., & Katzner, S. (2021). Evaluating visual cues modulates their representation in mouse visual and cingulate cortex. Journal of Neuroscience , 41 (15), 3531\u20133544. Wang, Y., Chiola, S., Yang, G., Russell, C., Armstrong, C. J., Wu, Y., Spampanato, J., Tarboton, P., Chang, A. N., Harmin, D. A.others. (2021). Modeling autism-associated SHANK3 deficiency using human cortico-striatal organoids generated from single neural rosettes. bioRxiv . 2020 Aguillon-Rodriguez, V., Angelaki, D. E., Bayer, H. M., Bonacchi, N., Carandini, M., Cazettes, F., Chapuis, G. A., Churchland, A. K., Dan, Y., Dewitt, E. E.others. (2020). A standardized and reproducible method to measure decision-making in mice. BioRxiv . Angelaki, D. E., Ng, J., Abrego, A. M., Cham, H. X., Asprodini, E. K., Dickman, J. D., & Laurens, J. (2020). A gravity-based three-dimensional compass in the mouse brain. Nature Communications , 11 (1), 1\u201313. Cotton, R. J., Sinz, F. H., & Tolias, A. S. (2020). Factorized neural processes for neural processes: K -shot prediction of neural responses. arXiv Preprint arXiv:2010.11810 . Heath, S. L., Christenson, M. P., Oriol, E., Saavedra-Weisenhaus, M., Kohn, J. R., & Behnia, R. (2020). Circuit mechanisms underlying chromatic encoding in drosophila photoreceptors. Current Biology . Laturnus, S., Kobak, D., & Berens, P. (2020). A systematic evaluation of interneuron morphology representations for cell type discrimination. Neuroinformatics , 18 (4), 591\u2013609. Sinz, F. H., Sachgau, C., Henninger, J., Benda, J., & Grewe, J. (2020). Simultaneous spike-time locking to multiple frequencies. Journal of Neurophysiology , 123 (6), 2355\u20132372. Yatsenko, D., Moreaux, L. C., Choi, J., Tolias, A., Shepard, K. L., & Roukes, M. L. (2020). Signal separability in integrated neurophotonics. bioRxiv . Zhao, Z., Klindt, D. A., Chagas, A. M., Szatko, K. P., Rogerson, L., Protti, D. A., Behrens, C., Dalkara, D., Schubert, T., Bethge, M.others. (2020). The temporal structure of the inner retina at a single glance. Scientific Reports , 10 (1), 1\u201317. 2019 Bonacchi, N., Chapuis, G., Churchland, A., Harris, K. D., Rossant, C., Sasaki, M., Shen, S., Steinmetz, N. A., Walker, E. Y., Winter, O.others. (2019). Data architecture and visualization for a large-scale neuroscience collaboration. BioRxiv , 827873. Cadena, S. A., Denfield, G. H., Walker, E. Y., Gatys, L. A., Tolias, A. S., Bethge, M., & Ecker, A. S. (2019). Deep convolutional models improve predictions of macaque V1 responses to natural images. PLoS Computational Biology , 15 (4), e1006897. Chettih, S. N., & Harvey, C. D. (2019). Single-neuron perturbations reveal feature-specific competition in V1. Nature , 567 (7748), 334\u2013340. Fahey, P. G., Muhammad, T., Smith, C., Froudarakis, E., Cobos, E., Fu, J., Walker, E. Y., Yatsenko, D., Sinz, F. H., Reimer, J.others. (2019). A global map of orientation tuning in mouse visual cortex. bioRxiv , 745323. Laurens, J., Abrego, A., Cham, H., Popeney, B., Yu, Y., Rotem, N., Aarse, J., Asprodini, E. K., Dickman, J. D., & Angelaki, D. E. (2019). Multiplexed code of navigation variables in anterior limbic areas. bioRxiv , 684464. Liu, G., Froudarakis, E., Patel, J. M., Kochukov, M. Y., Pekarek, B., Hunt, P. J., Patel, M., Ung, K., Fu, C.-H., Jo, J.others. (2019). Target specific functions of EPL interneurons in olfactory circuits. Nature Communications , 10 (1), 1\u201314. Ros\u00f3n, M. R., Bauer, Y., Kotkat, A. H., Berens, P., Euler, T., & Busse, L. (2019). Mouse dLGN receives functional input from a diverse population of retinal ganglion cells with limited convergence. Neuron , 102 (2), 462\u2013476. Spacek, M. A., Born, G., Crombie, D., Katzner, S., & Busse, L. (2019). Robust effects of cortical feedback on thalamic firing mode during naturalistic stimulation. BioRxiv , 776237. Walker, E. Y., Sinz, F. H., Cobos, E., Muhammad, T., Froudarakis, E., Fahey, P. G., Ecker, A. S., Reimer, J., Pitkow, X., & Tolias, A. S. (2019). Inception loops discover what excites neurons most using deep predictive models. Nature Neuroscience , 22 (12), 2060\u20132065. 2018 Denfield, G. H., Ecker, A. S., Shinn, T. J., Bethge, M., & Tolias, A. S. (2018). Attentional fluctuations induce shared variability in macaque primary visual cortex. Nature Communications , 9 (1), 2654. Ecker, A. S., Sinz, F. H., Froudarakis, E., Fahey, P. G., Cadena, S. A., Walker, E. Y., Cobos, E., Reimer, J., Tolias, A. S., & Bethge, M. (2018). A rotation-equivariant convolutional neural network model of primary visual cortex. arXiv Preprint arXiv:1809.10504 . Sinz, F., Ecker, A. S., Fahey, P., Walker, E., Cobos, E., Froudarakis, E., Yatsenko, D., Pitkow, Z., Reimer, J., & Tolias, A. (2018). Stimulus domain transfer in recurrent models for large scale cortical population prediction on video. Advances in Neural Information Processing Systems , 7199\u20137210. Walker, E. Y., Sinz, F. H., Froudarakis, E., Fahey, P. G., Muhammad, T., Ecker, A. S., Cobos, E., Reimer, J., Pitkow, X., & Tolias, A. S. (2018). Inception in visual cortex: In vivo-silico loops reveal most exciting images. bioRxiv , 506956. 2017 Franke, K., Berens, P., Schubert, T., Bethge, M., Euler, T., & Baden, T. (2017). Inhibition decorrelates visual feature representations in the inner retina. Nature , 542 (7642), 439. Jurjut, O., Georgieva, P., Busse, L., & Katzner, S. (2017). Learning enhances sensory processing in mouse V1 before improving behavior. Journal of Neuroscience , 37 (27), 6460\u20136474. Shan, K. Q., Lubenov, E. V., & Siapas, A. G. (2017). Model-based spike sorting with a mixture of drifting t-distributions. Journal of Neuroscience Methods , 288 , 82\u201398. 2016 Baden, T., Berens, P., Franke, K., Ros\u00f3n, M. R., Bethge, M., & Euler, T. (2016). The functional diversity of retinal ganglion cells in the mouse. Nature , 529 (7586), 345\u2013350. Cadwell, C. R., Palasantza, A., Jiang, X., Berens, P., Deng, Q., Yilmaz, M., Reimer, J., Shen, S., Bethge, M., Tolias, K. F.others. (2016). Electrophysiological, transcriptomic and morphologic profiling of single neurons using patch-seq. Nature Biotechnology , 34 (2), 199\u2013203. Hartmann, L., Drewe-Bo\u00df, P., Wie\u00dfner, T., Wagner, G., Geue, S., Lee, H.-C., Oberm\u00fcller, D. M., Kahles, A., Behr, J., Sinz, F. H.others. (2016). Alternative splicing substantially diversifies the transcriptome during early photomorphogenesis and correlates with the energy availability in arabidopsis. The Plant Cell , 28 (11), 2715\u20132734. Khastkhodaei, Z., Jurjut, O., Katzner, S., & Busse, L. (2016). Mice can use second-order, contrast-modulated stimuli to guide visual perception. Journal of Neuroscience , 36 (16), 4457\u20134469. Reimer, J., McGinley, M. J., Liu, Y., Rodenkirch, C., Wang, Q., McCormick, D. A., & Tolias, A. S. (2016). Pupil fluctuations track rapid changes in adrenergic and cholinergic activity in cortex. Nature Communications , 7 , 13289. Shan, K. Q., Lubenov, E. V., Papadopoulou, M., & Siapas, A. G. (2016). Spatial tuning and brain state account for dorsal hippocampal CA1 activity in a non-spatial learning task. eLife , 5 , e14321. 2015 Jiang, X., Shen, S., Cadwell, C. R., Berens, P., Sinz, F., Ecker, A. S., Patel, S., & Tolias, A. S. (2015). Principles of connectivity among morphologically defined cell types in adult neocortex. Science , 350 (6264), aac9462. Yatsenko, D., Josi\u0107, K., Ecker, A. S., Froudarakis, E., Cotton, R. J., & Tolias, A. S. (2015). Improved estimation and interpretation of correlations in neural circuits. PLoS Comput Biol , 11 (3), e1004083. 2014 Ecker, A. S., Berens, P., Cotton, R. J., Subramaniyan, M., Denfield, G. H., Cadwell, C. R., Smirnakis, S. M., Bethge, M., & Tolias, A. S. (2014). State dependence of noise correlations in macaque primary visual cortex. Neuron , 82 (1), 235\u2013248. Erisken, S., Vaiceliunaite, A., Jurjut, O., Fiorini, M., Katzner, S., & Busse, L. (2014). Effects of locomotion extend throughout the mouse early visual system. Current Biology , 24 (24), 2899\u20132907. Froudarakis, E., Berens, P., Ecker, A. S., Cotton, R. J., Sinz, F. H., Yatsenko, D., Saggau, P., Bethge, M., & Tolias, A. S. (2014). Population code in mouse V1 facilitates readout of natural scenes through increased sparseness. Nat Neurosci , 17 (6), 851\u2013857. Reimer, J., Froudarakis, E., Cadwell, C. R., Yatsenko, D., Denfield, G. H., & Tolias, A. S. (2014). Pupil fluctuations track fast switching of cortical states during quiet wakefulness . Neuron , 84 (2), 355\u2013362. Cotton, R. J., Froudarakis, E., Storer, P., Saggau, P., & Tolias, A. S. (2013). Three-dimensional mapping of microcircuit correlation structure. Frontiers in Neural Circuits , 7 , 151. 2013 Vaiceliunaite, A., Erisken, S., Franzen, F., Katzner, S., & Busse, L. (2013). Spatial integration in mouse primary visual cortex. Journal of Neurophysiology , 110 (4), 964\u2013972.","title":"Publications"},{"location":"community/contribute/","text":"Contribution Guidelines We have established contribution guidelines for DataJoint and DataJoint Elements.","title":"Contribution Guidelines"},{"location":"community/contribute/#contribution-guidelines","text":"We have established contribution guidelines for DataJoint and DataJoint Elements.","title":"Contribution Guidelines"},{"location":"community/nwb/","text":"NWB Integrations between DataJoint Elements and Neurodata Without Borders Aim DataJoint Elements and Neurodata Without Borders (NWB) are two neuroinformatics initiatives in active development. The projects develop independently yet they have complementary aims and overlapping user communities. This document establishes key processes for coordinating development and communications in order to promote integration and interoperability across the two ecosystems. Projects and Teams DataJoint DataJoint Elements \u2014 https://elements.datajoint.org \u2014 is a collection of open-source reference database schemas and analysis workflows for neurophysiology experiments, supported by DataJoint \u2014 https://datajoint.org \u2014 an open-source software framework. The project is funded by the NIH grant U24 NS116470 and led by Dr. Dimitri Yatsenko. The principal developer of DataJoint Elements and the DataJoint framework is the company DataJoint \u2014 https://datajoint.com . Neurodata without Borders (NWB) NWB - https://www.nwb.org \u2014 is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. The project is funded by the NIH grant U24 NS120057 and led by Dr. Oliver Rubel (Lawrence Berkeley National Laboratory) and Dr. Benjamin Dichter (Catalyst Neuro). The principal developers of NWB are the Lawrence Berkeley National Laboratory and Catalyst Neuro. General Principles No obligation The developers of the two ecosystems acknowledge that this roadmap document creates no contractual relationship between them but they agree to work together in the spirit of partnership to ensure that there is a united, visible, and responsive leadership and to demonstrate administrative and managerial commitment to coordinate development and communications. Coordinated Development The two projects will coordinate their development approaches to ensure maximum interoperability. This includes: coordinated use of terminology and nomenclatures support for testing infrastructure: unit testing and integration testing a coordinated software release process and versioning coordinated resolution of issues arising from joint use of the two tools Points of Contact To achieve the aims of coordinated development, both projects appoint a primary point of contact (POC) to respond to questions relating to the integration and interoperability of DataJoint Elements and NWB. For 2022, the DataJoint Elements POC is Dr. Chris Brozdowski For 2022, the NWB POC is Dr. Ryan Ly (Lawrence Berkeley National Laboratory) Annual Review To achieve the aims of coordinated development, the principal developers conduct a joint annual review of this roadmap document to ensure that the two programs are well integrated and not redundant. The contents and resolutions of the review will be made publicly available. Licensing The two parties ensure that relevant software components are developed under licenses that avoid any hindrance to integration and interoperability between DataJoint Elements workflows and NWB utilities.","title":"NWB"},{"location":"community/nwb/#nwb","text":"","title":"NWB"},{"location":"community/nwb/#integrations-between-datajoint-elements-and-neurodata-without-borders","text":"","title":"Integrations between DataJoint Elements and Neurodata Without Borders"},{"location":"community/nwb/#aim","text":"DataJoint Elements and Neurodata Without Borders (NWB) are two neuroinformatics initiatives in active development. The projects develop independently yet they have complementary aims and overlapping user communities. This document establishes key processes for coordinating development and communications in order to promote integration and interoperability across the two ecosystems.","title":"Aim"},{"location":"community/nwb/#projects-and-teams","text":"","title":"Projects and Teams"},{"location":"community/nwb/#datajoint","text":"DataJoint Elements \u2014 https://elements.datajoint.org \u2014 is a collection of open-source reference database schemas and analysis workflows for neurophysiology experiments, supported by DataJoint \u2014 https://datajoint.org \u2014 an open-source software framework. The project is funded by the NIH grant U24 NS116470 and led by Dr. Dimitri Yatsenko. The principal developer of DataJoint Elements and the DataJoint framework is the company DataJoint \u2014 https://datajoint.com .","title":"DataJoint"},{"location":"community/nwb/#neurodata-without-borders-nwb","text":"NWB - https://www.nwb.org \u2014 is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. The project is funded by the NIH grant U24 NS120057 and led by Dr. Oliver Rubel (Lawrence Berkeley National Laboratory) and Dr. Benjamin Dichter (Catalyst Neuro). The principal developers of NWB are the Lawrence Berkeley National Laboratory and Catalyst Neuro.","title":"Neurodata without Borders (NWB)"},{"location":"community/nwb/#general-principles","text":"","title":"General Principles"},{"location":"community/nwb/#no-obligation","text":"The developers of the two ecosystems acknowledge that this roadmap document creates no contractual relationship between them but they agree to work together in the spirit of partnership to ensure that there is a united, visible, and responsive leadership and to demonstrate administrative and managerial commitment to coordinate development and communications.","title":"No obligation"},{"location":"community/nwb/#coordinated-development","text":"The two projects will coordinate their development approaches to ensure maximum interoperability. This includes: coordinated use of terminology and nomenclatures support for testing infrastructure: unit testing and integration testing a coordinated software release process and versioning coordinated resolution of issues arising from joint use of the two tools","title":"Coordinated Development"},{"location":"community/nwb/#points-of-contact","text":"To achieve the aims of coordinated development, both projects appoint a primary point of contact (POC) to respond to questions relating to the integration and interoperability of DataJoint Elements and NWB. For 2022, the DataJoint Elements POC is Dr. Chris Brozdowski For 2022, the NWB POC is Dr. Ryan Ly (Lawrence Berkeley National Laboratory)","title":"Points of Contact"},{"location":"community/nwb/#annual-review","text":"To achieve the aims of coordinated development, the principal developers conduct a joint annual review of this roadmap document to ensure that the two programs are well integrated and not redundant. The contents and resolutions of the review will be made publicly available.","title":"Annual Review"},{"location":"community/nwb/#licensing","text":"The two parties ensure that relevant software components are developed under licenses that avoid any hindrance to integration and interoperability between DataJoint Elements workflows and NWB utilities.","title":"Licensing"},{"location":"community/support/","text":"Support We have a general policy for DataJoint open-source community engagement . General how-to questions: StackOverflow: datajoint tag Error questions: StackOverflow: datajoint tag Bug report: GitHub issue tracker New feature request: GitHub issue tracker Open-ended discussions: DataJoint Slack For fully managed services, we invite you to visit the DataJoint Solutions page and request more information.","title":"Support"},{"location":"community/support/#support","text":"We have a general policy for DataJoint open-source community engagement . General how-to questions: StackOverflow: datajoint tag Error questions: StackOverflow: datajoint tag Bug report: GitHub issue tracker New feature request: GitHub issue tracker Open-ended discussions: DataJoint Slack For fully managed services, we invite you to visit the DataJoint Solutions page and request more information.","title":"Support"},{"location":"description/animal/","text":"Description of modality, user population. Most pipeline begins with some information about the experiment animal subjects. This includes general information such as source, date of birth, sex, owner, and death information. In addition, many labs perform their own genotyping of the animal subjects while others rely on the animal care facility to perform it centrally. Most labs want to track the zygosity information of the animals. Those labs that perform their own genotyping need to keep track of additional information such as the breeding pairs, litters, weaning, caging, and genotyping tests. Precursor projects and interviews Over the past few years, several labs have developed DataJoint-based pipelines for animal management. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, associated tools, and interfaces. These teams include: International Brain Laboratory https://github.com/int-brain-lab/IBL-pipeline BrainCoGs (Princeton Neuroscience Institute) https://github.com/BrainCOGS/U19-pipeline_python ; https://github.com/BrainCOGS/U19-pipeline-matlab MoC3 (Columbia Zuckerman Institute) + Costa Lab (private repository) + Hillman Lab: https://github.com/ZuckermanBrain/datajoint-hillman Development and validation Through our interviews and direct collaboration with the precursor projects, we identified the common motifs in the animal subject schemas to create the Animal Management Element. This element works for diverse downstream pipelines and is always used in combination with other elements for specific experiments. As such it is validated jointly with the processing elements such as the Array Ephys Element and Calcium Imaging Element.","title":"Animal"},{"location":"description/animal/#description-of-modality-user-population","text":"Most pipeline begins with some information about the experiment animal subjects. This includes general information such as source, date of birth, sex, owner, and death information. In addition, many labs perform their own genotyping of the animal subjects while others rely on the animal care facility to perform it centrally. Most labs want to track the zygosity information of the animals. Those labs that perform their own genotyping need to keep track of additional information such as the breeding pairs, litters, weaning, caging, and genotyping tests.","title":"Description of modality, user population."},{"location":"description/animal/#precursor-projects-and-interviews","text":"Over the past few years, several labs have developed DataJoint-based pipelines for animal management. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, associated tools, and interfaces. These teams include: International Brain Laboratory https://github.com/int-brain-lab/IBL-pipeline BrainCoGs (Princeton Neuroscience Institute) https://github.com/BrainCOGS/U19-pipeline_python ; https://github.com/BrainCOGS/U19-pipeline-matlab MoC3 (Columbia Zuckerman Institute) + Costa Lab (private repository) + Hillman Lab: https://github.com/ZuckermanBrain/datajoint-hillman","title":"Precursor projects and interviews"},{"location":"description/animal/#development-and-validation","text":"Through our interviews and direct collaboration with the precursor projects, we identified the common motifs in the animal subject schemas to create the Animal Management Element. This element works for diverse downstream pipelines and is always used in combination with other elements for specific experiments. As such it is validated jointly with the processing elements such as the Array Ephys Element and Calcium Imaging Element.","title":"Development and validation"},{"location":"description/array_ephys/","text":"Array Electrophysiology Description of modality, user population Neuropixels probes were developed by a collaboration between HHMI Janelia, industry partners, and others (Jun et al., Nature 2017). Since its initial release in October 2018, 300 labs have ordered 1200 probes. Since the rollout of Neuropixels 2.0 in October 2020, IMEC has been shipping 100+ probes monthly (correspondence with Tim Harris). Neuropixels probes offer 960 electrode sites along a 10mm long shank, with 384 recordable channels per probe that can record hundreds of units spanning multiple brain regions (Neuropixels 2.0 version is a 4-shank probe with 1280 electrode sites per shank). Such large recording capacity have offered tremendous opportunities for the field of neurophysiology research, yet this is accompanied by an equally great challenge in terms of data and computation management. Acquisition tools The typical instrumentation used for data acquisition is the Neuropixels probe and headstage interfacing with a PXIe acquisition module ( https://www.neuropixels.org/control-system ). Two main acquisition softwares are used for Neuropixels: + SpikeGLX - developed by Bill Karsh and Tim Harris at HHMI/Janelia + OpenEphys - developed by Joshua Siegle at the Allen Institute. These save the data into specific directory structure and file-naming convention as custom binary formats (e.g. \u201c.bin\u201d, \u201c.dat\u201d). Meta data are stored as separate files in xml or text format. Preprocessing tools The preprocessing pipeline includes bandpass filtering for LFP extraction, bandpass filtering for spike sorting, spike sorting, and manual curation of the spike sorting results, and calculation of quality control metrics. In trial-based experiments, the spike trains are aligned and separated into trials. Standard processing may include PSTH computation aligned to trial onset or other events, and often grouped by different trial types. Neuroscience groups have traditionally developed custom home-made toolchains. In recent years, several leaders have been emerging as de facto standards with significant community uptake: + Kilosort + JRClust + MountainSort + SpyKING CIRCUS Kilosort provides most automation and has gained significant popularity, being adopted as one of the key spike sorting methods in the majority of the teams/collaborations we have worked with. As part of Year-1 U24 effort, we provide support for data ingestion of spike sorting results from Kilosort. Further effort will be devoted for the ingestion support of other spike sorting methods. On this end, a framework for unifying existing spike sorting methods, named SpikeInterface , has been developed by Alessio Buccino, et al. SpikeInterface provides a convenient Python-based wrapper to invoke, extract, compare spike sorting results from different sorting algorithms. Key projects Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for Neuropixels probes. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, pipeline design, associated tools, and interfaces. These teams include: International Brain Lab - https://internationalbrainlab.org - https://github.com/int-brain-lab/IBL-pipeline Mesoscale Activity Project (HHMI Janelia) - https://github.com/mesoscale-activity-map - https://github.com/mesoscale-activity-map/map-ephys Moser Group (private repository) Andreas Tolias Lab (BCM) BrainCoGs (Princeton Neuroscience Institute) Brody Lab (Princeton) Pipeline Development Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the Array Electrophysiology Element with the repository hosted at https://github.com/datajoint/element-array-ephys . Major features of the Array Electrophysiology Element include: + Pipeline architecture defining: + Probe, electrode configuration compatible with Neuropixels probes and generalizable to other types of probes (e.g. tetrodes) - supporting both chronic and acute probe insertion mode + Probe-insertion, ephys-recordings, LFP extraction, clusterings, curations, sorted units and the associated data (e.g. spikes, waveforms, etc.) + Store/track/manage different curations of the spike sorting results + Ingestion support for data acquired with SpikeGLX and OpenEphys acquisition systems + Ingestion support for spike sorting outputs from Kilosort + Sample data and complete test suite for quality assurance Incorporation of SpikeInterface into the Array Electrophysiology Element will be on DataJoint Elements development roadmap. Dr. Loren Frank has led a development effort of a DataJoint pipeline with SpikeInterface framework and NeurodataWithoutBorders format integrated ( https://github.com/LorenFrankLab/nwb_datajoint ).","title":"Array Electrophysiology"},{"location":"description/array_ephys/#array-electrophysiology","text":"","title":"Array Electrophysiology"},{"location":"description/array_ephys/#description-of-modality-user-population","text":"Neuropixels probes were developed by a collaboration between HHMI Janelia, industry partners, and others (Jun et al., Nature 2017). Since its initial release in October 2018, 300 labs have ordered 1200 probes. Since the rollout of Neuropixels 2.0 in October 2020, IMEC has been shipping 100+ probes monthly (correspondence with Tim Harris). Neuropixels probes offer 960 electrode sites along a 10mm long shank, with 384 recordable channels per probe that can record hundreds of units spanning multiple brain regions (Neuropixels 2.0 version is a 4-shank probe with 1280 electrode sites per shank). Such large recording capacity have offered tremendous opportunities for the field of neurophysiology research, yet this is accompanied by an equally great challenge in terms of data and computation management.","title":"Description of modality, user population"},{"location":"description/array_ephys/#acquisition-tools","text":"The typical instrumentation used for data acquisition is the Neuropixels probe and headstage interfacing with a PXIe acquisition module ( https://www.neuropixels.org/control-system ). Two main acquisition softwares are used for Neuropixels: + SpikeGLX - developed by Bill Karsh and Tim Harris at HHMI/Janelia + OpenEphys - developed by Joshua Siegle at the Allen Institute. These save the data into specific directory structure and file-naming convention as custom binary formats (e.g. \u201c.bin\u201d, \u201c.dat\u201d). Meta data are stored as separate files in xml or text format.","title":"Acquisition tools"},{"location":"description/array_ephys/#preprocessing-tools","text":"The preprocessing pipeline includes bandpass filtering for LFP extraction, bandpass filtering for spike sorting, spike sorting, and manual curation of the spike sorting results, and calculation of quality control metrics. In trial-based experiments, the spike trains are aligned and separated into trials. Standard processing may include PSTH computation aligned to trial onset or other events, and often grouped by different trial types. Neuroscience groups have traditionally developed custom home-made toolchains. In recent years, several leaders have been emerging as de facto standards with significant community uptake: + Kilosort + JRClust + MountainSort + SpyKING CIRCUS Kilosort provides most automation and has gained significant popularity, being adopted as one of the key spike sorting methods in the majority of the teams/collaborations we have worked with. As part of Year-1 U24 effort, we provide support for data ingestion of spike sorting results from Kilosort. Further effort will be devoted for the ingestion support of other spike sorting methods. On this end, a framework for unifying existing spike sorting methods, named SpikeInterface , has been developed by Alessio Buccino, et al. SpikeInterface provides a convenient Python-based wrapper to invoke, extract, compare spike sorting results from different sorting algorithms.","title":"Preprocessing tools"},{"location":"description/array_ephys/#key-projects","text":"Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for Neuropixels probes. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, pipeline design, associated tools, and interfaces. These teams include: International Brain Lab - https://internationalbrainlab.org - https://github.com/int-brain-lab/IBL-pipeline Mesoscale Activity Project (HHMI Janelia) - https://github.com/mesoscale-activity-map - https://github.com/mesoscale-activity-map/map-ephys Moser Group (private repository) Andreas Tolias Lab (BCM) BrainCoGs (Princeton Neuroscience Institute) Brody Lab (Princeton)","title":"Key projects"},{"location":"description/array_ephys/#pipeline-development","text":"Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the Array Electrophysiology Element with the repository hosted at https://github.com/datajoint/element-array-ephys . Major features of the Array Electrophysiology Element include: + Pipeline architecture defining: + Probe, electrode configuration compatible with Neuropixels probes and generalizable to other types of probes (e.g. tetrodes) - supporting both chronic and acute probe insertion mode + Probe-insertion, ephys-recordings, LFP extraction, clusterings, curations, sorted units and the associated data (e.g. spikes, waveforms, etc.) + Store/track/manage different curations of the spike sorting results + Ingestion support for data acquired with SpikeGLX and OpenEphys acquisition systems + Ingestion support for spike sorting outputs from Kilosort + Sample data and complete test suite for quality assurance Incorporation of SpikeInterface into the Array Electrophysiology Element will be on DataJoint Elements development roadmap. Dr. Loren Frank has led a development effort of a DataJoint pipeline with SpikeInterface framework and NeurodataWithoutBorders format integrated ( https://github.com/LorenFrankLab/nwb_datajoint ).","title":"Pipeline Development"},{"location":"description/calcium_imaging/","text":"Calcium Imaging Description of modality, user population Over the past two decades, in vivo two-photon laser-scanning imaging of calcium signals has evolved into a mainstream modality for neurophysiology experiments to record population activity in intact neural circuits. The tools for signal acquisition and analysis continue to evolve but common patterns and elements of standardization have emerged. Acquisition tools Hardware The primary acquisition systems are: + Sutter (we estimate 400 rigs in active use - TBC) + Thorlabs (we estimate 400 rigs in active use - TBC) + Bruker (we estimate 400 rigs in active use - TBC) + Neurolabware (we estimate 400 rigs in active use - TBC) We do not include Miniscopes in these estimates. In all there are perhaps on the order of 3000 two-photon setups globally but their processing needs may need to be further segmented. Software ScanImage ThorImageLS Scanbox Vidrio\u2019s ScanImage is the data acquisition software for two types of home-built scanning two-photon systems, either based on Thorlabs and Sutter hardware. ScanImage has a free version and a licensed version. Thorlabs also provides their own acquisition software - ThorImageLS (probably half of the systems). Preprocessing toolchain: development teams The preprocessing workflow for two-photon laser-scanning microscopy includes motion correction (rigid or non-rigid), cell segmentation, and calcium event extraction (sometimes described as \"deconvolution\" or \"spike inference\"). Some include raster artifact correction, cropping and stitching operations. Until recently, most labs have developed custom processing pipelines, sharing them with others as academic open-source projects. Recently, a few leaders have emerged as standardization candidates for the initial preprocessing. CaImAn (Originally developed by Andrea Giovannucci, current support by FlatIron Institute: Eftychios A. Pnevmatikakis, Johannes Friedrich) Suite2p (Carsen Stringer and Marius Pachitariu at Janelia), 200+ users, active support Key projects Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for two-photon Calcium imaging. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, pipeline design, associated tools, and interfaces. These teams include: + MICrONS (Andreas Tolias Lab, BCM) - https://github.com/cajal + BrainCoGs (Princeton) - https://github.com/BrainCOGS + Moser Group (Kavli Institute/NTNU) - private repository + Anne Churchland Lab (UCLA) Pipeline Development Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the Calcium ImagingElement with the repository hosted at https://github.com/datajoint/element-calcium-imaging . Major features of the Calcium Imaging Element include: + Pipeline architecture defining: + Calcium-imaging scanning metadata, also compatible with mesoscale imaging and multi-ROI scanning mode + Tables for all processing steps: motion correction, segmentation, cell spatial footprint, fluorescence trace extraction, spike inference and cell classification + Store/track/manage different curations of the segmentation results + Ingestion support for data acquired with ScanImage and Scanbox acquisition systems + Ingestion support for processing outputs from both Suite2p and CaImAn analysis suites + Sample data and complete test suite for quality assurance The processing workflow is typically performed on a per-scan basis, however, depending on the nature of the research questions, different labs may opt to perform processing/segmentation on a concatenated set of data from multiple scans. To this end, we have extended the Calcium Imaging Element and provided a design version capable of supporting a multi-scan processing scheme.","title":"Calcium Imaging"},{"location":"description/calcium_imaging/#calcium-imaging","text":"","title":"Calcium Imaging"},{"location":"description/calcium_imaging/#description-of-modality-user-population","text":"Over the past two decades, in vivo two-photon laser-scanning imaging of calcium signals has evolved into a mainstream modality for neurophysiology experiments to record population activity in intact neural circuits. The tools for signal acquisition and analysis continue to evolve but common patterns and elements of standardization have emerged.","title":"Description of modality, user population"},{"location":"description/calcium_imaging/#acquisition-tools","text":"","title":"Acquisition tools"},{"location":"description/calcium_imaging/#hardware","text":"The primary acquisition systems are: + Sutter (we estimate 400 rigs in active use - TBC) + Thorlabs (we estimate 400 rigs in active use - TBC) + Bruker (we estimate 400 rigs in active use - TBC) + Neurolabware (we estimate 400 rigs in active use - TBC) We do not include Miniscopes in these estimates. In all there are perhaps on the order of 3000 two-photon setups globally but their processing needs may need to be further segmented.","title":"Hardware"},{"location":"description/calcium_imaging/#software","text":"ScanImage ThorImageLS Scanbox Vidrio\u2019s ScanImage is the data acquisition software for two types of home-built scanning two-photon systems, either based on Thorlabs and Sutter hardware. ScanImage has a free version and a licensed version. Thorlabs also provides their own acquisition software - ThorImageLS (probably half of the systems).","title":"Software"},{"location":"description/calcium_imaging/#preprocessing-toolchain-development-teams","text":"The preprocessing workflow for two-photon laser-scanning microscopy includes motion correction (rigid or non-rigid), cell segmentation, and calcium event extraction (sometimes described as \"deconvolution\" or \"spike inference\"). Some include raster artifact correction, cropping and stitching operations. Until recently, most labs have developed custom processing pipelines, sharing them with others as academic open-source projects. Recently, a few leaders have emerged as standardization candidates for the initial preprocessing. CaImAn (Originally developed by Andrea Giovannucci, current support by FlatIron Institute: Eftychios A. Pnevmatikakis, Johannes Friedrich) Suite2p (Carsen Stringer and Marius Pachitariu at Janelia), 200+ users, active support","title":"Preprocessing toolchain: development teams"},{"location":"description/calcium_imaging/#key-projects","text":"Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for two-photon Calcium imaging. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, pipeline design, associated tools, and interfaces. These teams include: + MICrONS (Andreas Tolias Lab, BCM) - https://github.com/cajal + BrainCoGs (Princeton) - https://github.com/BrainCOGS + Moser Group (Kavli Institute/NTNU) - private repository + Anne Churchland Lab (UCLA)","title":"Key projects"},{"location":"description/calcium_imaging/#pipeline-development","text":"Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the Calcium ImagingElement with the repository hosted at https://github.com/datajoint/element-calcium-imaging . Major features of the Calcium Imaging Element include: + Pipeline architecture defining: + Calcium-imaging scanning metadata, also compatible with mesoscale imaging and multi-ROI scanning mode + Tables for all processing steps: motion correction, segmentation, cell spatial footprint, fluorescence trace extraction, spike inference and cell classification + Store/track/manage different curations of the segmentation results + Ingestion support for data acquired with ScanImage and Scanbox acquisition systems + Ingestion support for processing outputs from both Suite2p and CaImAn analysis suites + Sample data and complete test suite for quality assurance The processing workflow is typically performed on a per-scan basis, however, depending on the nature of the research questions, different labs may opt to perform processing/segmentation on a concatenated set of data from multiple scans. To this end, we have extended the Calcium Imaging Element and provided a design version capable of supporting a multi-scan processing scheme.","title":"Pipeline Development"},{"location":"description/deeplabcut/","text":"In development","title":"Deeplabcut"},{"location":"description/electrode_localization/","text":"In development","title":"Electrode localization"},{"location":"description/event/","text":"In development","title":"Event"},{"location":"description/facemap/","text":"In development","title":"Facemap"},{"location":"description/lab/","text":"Description of modality, user population Most pipelines track some information about the lab, including the facilities, experiment rigs, and users. All interviewed labs have some version of these elements. They also have custom interfaces and GUIs for data entry. Precursor projects and interviews Over the past few years, several labs have developed DataJoint-based pipelines for lab management. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, associated tools, and interfaces. These teams include: + International Brain Laboratory + BrainCoGs (Princeton Neuroscience Institute), Python pipeline , MATLAB pipeline + MoC3 (Columbia Zuckerman Institute) + Churchland Lab + Costa Lab (private repository) + Hillman Lab Development and validation Through our interviews and direct collaboration on the precursor projects, we identified the common motifs in the lab schemas to create the Lab Management Element. This element works for diverse downstream pipelines and is always used in combination with other elements for specific experiments. As such, it is validated jointly with the acquisition elements such as the Extracellular Array Electrophysiology Element and Calcium Imaging Element .","title":"Lab"},{"location":"description/lab/#description-of-modality-user-population","text":"Most pipelines track some information about the lab, including the facilities, experiment rigs, and users. All interviewed labs have some version of these elements. They also have custom interfaces and GUIs for data entry.","title":"Description of modality, user population"},{"location":"description/lab/#precursor-projects-and-interviews","text":"Over the past few years, several labs have developed DataJoint-based pipelines for lab management. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, associated tools, and interfaces. These teams include: + International Brain Laboratory + BrainCoGs (Princeton Neuroscience Institute), Python pipeline , MATLAB pipeline + MoC3 (Columbia Zuckerman Institute) + Churchland Lab + Costa Lab (private repository) + Hillman Lab","title":"Precursor projects and interviews"},{"location":"description/lab/#development-and-validation","text":"Through our interviews and direct collaboration on the precursor projects, we identified the common motifs in the lab schemas to create the Lab Management Element. This element works for diverse downstream pipelines and is always used in combination with other elements for specific experiments. As such, it is validated jointly with the acquisition elements such as the Extracellular Array Electrophysiology Element and Calcium Imaging Element .","title":"Development and validation"},{"location":"description/miniscope/","text":"Miniscope Description of modality, user population Miniature fluorescence microscopes (miniscopes) are a head-mounted calcium imaging full-frame video modality first introduced in 2005 by Mark Schnitzer's lab ( Flusberg et al., Optics Letters 2005 ). Due to their light weight, these miniscopes allow measuring the dynamic activity of populations of cortical neurons in freely behaving animals. In 2011, Inscopix Inc. was founded to support one-photon miniscopes as a commercial neuroscience research platform, providing proprietary hardware, acquisition software, and analysis software. Today, they estimate their active user base is 491 labs with a total of 1179 installs. An open-source alternative was launched by a UCLA team led by Daniel Aharoni and Peyman Golshani ( Cai et al., Nature 2016 ; Aharoni and Hoogland, Frontiers in Cellular Neuroscience 2019 ). In our conversation with Dr. Aharoni, he estimated about 700 labs currently using the UCLA system alone. The Inscopix user base is smaller but more established. Several two-photon miniscopes have been developed but lack widespread adoption likely due to the expensive hardware required for the two-photon excitation ( Helmchen et al., Neuron 2001 ; Zong et al., Nature Methods 2017 ; Aharoni and Hoogland, Frontiers in Cellular Neuroscience 2019 ). Due to the low costs and ability to record during natural behaviors, one-photon miniscope imaging appears to be the fastest growing calcium imaging modality in the field today. In Year 1, we focused our efforts on supporting the UCLA platform due its fast growth and deficiency of standardization in acquisition and processing pipelines. In future phases, we will reach out to Inscopix to support their platform as well. Acquisition tools Daniel Aharoni's lab has developed iterations of the UCLA Miniscope platform. Based on interviews, we have found labs using the two most recent versions including Miniscope DAQ V3 and Miniscope DAQ V4 . Labs also use the Bonsai OpenEphys tool for data acquisition with the UCLA miniscope. Inscopix provides the Inscopix Data Acquisition Software (IDAS) for the nVista and nVoke systems. Preprocessing tools The preprocessing workflow for miniscope imaging includes denoising, motion correction, cell segmentation, and calcium event extraction (sometimes described as \"deconvolution\" or \"spike inference\"). For the UCLA Miniscopes, the following analysis packages are commonly used: (Package, Developer [Affiliation], Programming Language) Miniscope Denoising , Daniel Aharoni (UCLA), Python NoRMCorre , Flatiron Institute, MATLAB CNMF-E , Pengcheng Zhou (Liam Paninski\u2019s Lab, Columbia University), MATLAB CaImAn , Flatiron Institute, Python miniscoPy , Guillaume Viejo (Adrien Peyrache\u2019s Lab, McGill University), Python MIN1PIPE , Jinghao Lu (Fan Wang\u2019s Lab, MIT), MATLAB CIAtah , Biafra Ahanonu, MATLAB MiniAn , Phil Dong (Denise Cai's Lab, Mount Sinai), Python MiniscopeAnalysis , Guillaume Etter (Sylvain Williams\u2019 Lab, McGill University), MATLAB PIMPN , Guillaume Etter (Sylvain Williams\u2019 Lab, McGill University), Python CellReg , Liron Sheintuch (Yaniv Ziv\u2019s Lab, Weizmann Institute of Science), MATLAB Inscopix Data Processing Software (IDPS) Inscopix Multimodal Image Registration and Analysis (MIRA) Based on interviews with UCLA and Inscopix miniscope users and developers, each research lab uses a different preprocessing workflow. These custom workflows are often closed source and not tracked with version control software. For the preprocessing tools that are open source, they are often developed by an individual during their training period and lack funding for long term maintenance. These factors result in a lack of standardization for miniscope preprocessing tools, which is a major obstacle to adoption for new labs. Key projects Until recently, DataJoint had not been used for miniscope pipelines. However, labs we have contacted have been eager to engage and adopt DataJoint-based workflows in their labs. Adrien Peyrache Lab, McGill University Peyman Golshani Lab, UCLA Daniel Aharoni Lab, UCLA Anne Churchland Lab, UCLA Fan Wang Lab, MIT Antoine Adamantidis Lab, University of Bern Manolis Froudaraki Lab, FORTH Allan Basbaum Lab, UCSF Pipeline Development With assistance from Peyman Golshani\u2019s Lab (UCLA) we have added support for the UCLA Miniscope DAQ V3 acquisition tool and MiniscopeAnalysis preprocessing tool in element-miniscope and workflow-miniscope . They have provided example data for development, and will begin validating in March 2021. Based on interviews, we are considering adding support for the tools listed below. The deciding factors include the number of users, long term support, quality controls, and python programming language (so that the preprocessing tool can be triggered within the element). Acquisition tools + Miniscope DAQ V4 + Inscopix Data Acquisition Software (IDAS) Preprocessing tools + Inscopix Data Processing Software (IDPS) + Inscopix Multimodal Image Registration and Analysis (MIRA) + MiniAn + CaImAn + CNMF-E + CellReg","title":"Miniscope"},{"location":"description/miniscope/#miniscope","text":"","title":"Miniscope"},{"location":"description/miniscope/#description-of-modality-user-population","text":"Miniature fluorescence microscopes (miniscopes) are a head-mounted calcium imaging full-frame video modality first introduced in 2005 by Mark Schnitzer's lab ( Flusberg et al., Optics Letters 2005 ). Due to their light weight, these miniscopes allow measuring the dynamic activity of populations of cortical neurons in freely behaving animals. In 2011, Inscopix Inc. was founded to support one-photon miniscopes as a commercial neuroscience research platform, providing proprietary hardware, acquisition software, and analysis software. Today, they estimate their active user base is 491 labs with a total of 1179 installs. An open-source alternative was launched by a UCLA team led by Daniel Aharoni and Peyman Golshani ( Cai et al., Nature 2016 ; Aharoni and Hoogland, Frontiers in Cellular Neuroscience 2019 ). In our conversation with Dr. Aharoni, he estimated about 700 labs currently using the UCLA system alone. The Inscopix user base is smaller but more established. Several two-photon miniscopes have been developed but lack widespread adoption likely due to the expensive hardware required for the two-photon excitation ( Helmchen et al., Neuron 2001 ; Zong et al., Nature Methods 2017 ; Aharoni and Hoogland, Frontiers in Cellular Neuroscience 2019 ). Due to the low costs and ability to record during natural behaviors, one-photon miniscope imaging appears to be the fastest growing calcium imaging modality in the field today. In Year 1, we focused our efforts on supporting the UCLA platform due its fast growth and deficiency of standardization in acquisition and processing pipelines. In future phases, we will reach out to Inscopix to support their platform as well.","title":"Description of modality, user population"},{"location":"description/miniscope/#acquisition-tools","text":"Daniel Aharoni's lab has developed iterations of the UCLA Miniscope platform. Based on interviews, we have found labs using the two most recent versions including Miniscope DAQ V3 and Miniscope DAQ V4 . Labs also use the Bonsai OpenEphys tool for data acquisition with the UCLA miniscope. Inscopix provides the Inscopix Data Acquisition Software (IDAS) for the nVista and nVoke systems.","title":"Acquisition tools"},{"location":"description/miniscope/#preprocessing-tools","text":"The preprocessing workflow for miniscope imaging includes denoising, motion correction, cell segmentation, and calcium event extraction (sometimes described as \"deconvolution\" or \"spike inference\"). For the UCLA Miniscopes, the following analysis packages are commonly used: (Package, Developer [Affiliation], Programming Language) Miniscope Denoising , Daniel Aharoni (UCLA), Python NoRMCorre , Flatiron Institute, MATLAB CNMF-E , Pengcheng Zhou (Liam Paninski\u2019s Lab, Columbia University), MATLAB CaImAn , Flatiron Institute, Python miniscoPy , Guillaume Viejo (Adrien Peyrache\u2019s Lab, McGill University), Python MIN1PIPE , Jinghao Lu (Fan Wang\u2019s Lab, MIT), MATLAB CIAtah , Biafra Ahanonu, MATLAB MiniAn , Phil Dong (Denise Cai's Lab, Mount Sinai), Python MiniscopeAnalysis , Guillaume Etter (Sylvain Williams\u2019 Lab, McGill University), MATLAB PIMPN , Guillaume Etter (Sylvain Williams\u2019 Lab, McGill University), Python CellReg , Liron Sheintuch (Yaniv Ziv\u2019s Lab, Weizmann Institute of Science), MATLAB Inscopix Data Processing Software (IDPS) Inscopix Multimodal Image Registration and Analysis (MIRA) Based on interviews with UCLA and Inscopix miniscope users and developers, each research lab uses a different preprocessing workflow. These custom workflows are often closed source and not tracked with version control software. For the preprocessing tools that are open source, they are often developed by an individual during their training period and lack funding for long term maintenance. These factors result in a lack of standardization for miniscope preprocessing tools, which is a major obstacle to adoption for new labs.","title":"Preprocessing tools"},{"location":"description/miniscope/#key-projects","text":"Until recently, DataJoint had not been used for miniscope pipelines. However, labs we have contacted have been eager to engage and adopt DataJoint-based workflows in their labs. Adrien Peyrache Lab, McGill University Peyman Golshani Lab, UCLA Daniel Aharoni Lab, UCLA Anne Churchland Lab, UCLA Fan Wang Lab, MIT Antoine Adamantidis Lab, University of Bern Manolis Froudaraki Lab, FORTH Allan Basbaum Lab, UCSF","title":"Key projects"},{"location":"description/miniscope/#pipeline-development","text":"With assistance from Peyman Golshani\u2019s Lab (UCLA) we have added support for the UCLA Miniscope DAQ V3 acquisition tool and MiniscopeAnalysis preprocessing tool in element-miniscope and workflow-miniscope . They have provided example data for development, and will begin validating in March 2021. Based on interviews, we are considering adding support for the tools listed below. The deciding factors include the number of users, long term support, quality controls, and python programming language (so that the preprocessing tool can be triggered within the element). Acquisition tools + Miniscope DAQ V4 + Inscopix Data Acquisition Software (IDAS) Preprocessing tools + Inscopix Data Processing Software (IDPS) + Inscopix Multimodal Image Registration and Analysis (MIRA) + MiniAn + CaImAn + CNMF-E + CellReg","title":"Pipeline Development"},{"location":"description/session/","text":"Session Description of modality, user population Session information is part of most data modalities. This is a minimal schema with a few number of tables describing the experiment session (date and time, experimenter, subject reference, experiment rig, aims, and notes), the project in which the sessions may belong to (project description, DOI, keywords, etc.), data directory for each session. Precursor projects and interviews All DataJoint pipelines have some form of a session schema or tables. The session table is typically in the upstream part of the pipeline, referencing the subject and serves as a common node for other modalities to connect to and expand downstream (e.g. ephys, imaging, video tracking, behavioral events, optogenetic perturbation, etc.). Development We developed the Session Element under https://github.com/datajoint/element-session . This schema is validated as part of complete workflows in the specific modalities.","title":"Session"},{"location":"description/session/#session","text":"","title":"Session"},{"location":"description/session/#description-of-modality-user-population","text":"Session information is part of most data modalities. This is a minimal schema with a few number of tables describing the experiment session (date and time, experimenter, subject reference, experiment rig, aims, and notes), the project in which the sessions may belong to (project description, DOI, keywords, etc.), data directory for each session.","title":"Description of modality, user population"},{"location":"description/session/#precursor-projects-and-interviews","text":"All DataJoint pipelines have some form of a session schema or tables. The session table is typically in the upstream part of the pipeline, referencing the subject and serves as a common node for other modalities to connect to and expand downstream (e.g. ephys, imaging, video tracking, behavioral events, optogenetic perturbation, etc.).","title":"Precursor projects and interviews"},{"location":"description/session/#development","text":"We developed the Session Element under https://github.com/datajoint/element-session . This schema is validated as part of complete workflows in the specific modalities.","title":"Development"},{"location":"description/visual_stimulus/","text":"In development","title":"Visual stimulus"},{"location":"management/governance/","text":"Project Governance Funding This Resource is supported by the National Institute Of Neurological Disorders And Stroke of the National Institutes of Health under Award Number U24NS116470. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. Scientific Steering Group The project oversight and guidance is provided by the Scientific Steering Group comprising Mackenzie Mathis (EPFL) John Cunningham (Columbia U) Carlos Brody (Princeton U) Karel Svoboda (Allen Institute) Nick Steinmetz (U of Washington) Loren Frank (UCSF)","title":"Project Governance"},{"location":"management/governance/#project-governance","text":"","title":"Project Governance"},{"location":"management/governance/#funding","text":"This Resource is supported by the National Institute Of Neurological Disorders And Stroke of the National Institutes of Health under Award Number U24NS116470. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.","title":"Funding"},{"location":"management/governance/#scientific-steering-group","text":"The project oversight and guidance is provided by the Scientific Steering Group comprising Mackenzie Mathis (EPFL) John Cunningham (Columbia U) Carlos Brody (Princeton U) Karel Svoboda (Allen Institute) Nick Steinmetz (U of Washington) Loren Frank (UCSF)","title":"Scientific Steering Group"},{"location":"management/licenses/","text":"Licenses and User Agreements All components of provided by the Resource are distributed under permissive open-source licenses. These licenses are included in the respective repositories and are summarized below: DataJoint for Python: LGPLv3 License DataJoint for MATLAB: MIT License Pharus: MIT License DataJoint LabBook: MIT License DataJoint Elements and Workflows: MIT License","title":"Licenses and User Agreements"},{"location":"management/licenses/#licenses-and-user-agreements","text":"All components of provided by the Resource are distributed under permissive open-source licenses. These licenses are included in the respective repositories and are summarized below: DataJoint for Python: LGPLv3 License DataJoint for MATLAB: MIT License Pharus: MIT License DataJoint LabBook: MIT License DataJoint Elements and Workflows: MIT License","title":"Licenses and User Agreements"},{"location":"management/outreach/","text":"Outreach Plan Broad engagement with the neuroscience community is necessary for the optimization, integration, and adoption of the Resource components. We conduct five types of outreach activities that require different approaches: 1. Precursor Projects Our Selection Process requires a \"Precursor Project\" for any new experiment modality to be included in DataJoint Elements. A precursor project is a project that develops a DataJoint pipeline for its own experiments either independently or in collaboration with our team. We reach out to teams who develop DataJoint pipelines for new experiment paradigms and modalities to identify essential design motifs, analysis tools, and related tools and interfaces. We interview the core team to learn about their collaborative culture, practices, and procedures. We jointly review their open-source code and their plans for disseminating it. In many cases, our team already collaborates with such teams through our other projects and we have a good understanding of their process. As we develop a new Element to support the new modality, we remain in contact with the team to include their contribution, solicit feedback, and evaluate design tradeoffs, When the new Element is released, a full attribution is given to the Precursor Project. Rationale: The Resource does not aim to develop fundamentally new solutions for neurophysiology data acquisition and analysis. Rather it aims to systematize and disseminate existing open-source tools proven in leading research projects. 2. Tool Developers DataJoint pipelines rely on analysis tools, atlases, data standards, archives and catalogs, and other neuroinformatics resources developed and maintained by the broader scientific community. To ensure sustainability of the Resource, we reach out to the tool developer to establish joint sustainability roadmaps. 3. Validation Sites Before new Resource components are released as part of this Resource, our team engages other research groups to evaluate these components in their projects in realistic scenarios experiments. During the Alpha testing period, the validation sites set up the workflow by either integrating Resource components with their existing workflow or by combining only the Resource components with the help of our team. We evaluate the feedback and upgrade our Resource components accordingly during this process. 4. Dissemination We conduct activities to disseminate Resource components for adoption in diverse neuroscience labs. These activities include A central website for the Resource. Presently, this site serves this purpose. Conference talks, presentations, and workshops Publications in peer-reviewed journals White papers posted on internet resources and websites On-site workshops by invitation Remote workshops and webinars Online interactive tutorials hosted on DataJoint CodeBook . 5. Census In order to measure the effectiveness of the Resource, we conduct several activities to estimate the adoption and use of the Resource components: A citation mechanism for individual components of the Resource. Resource RRID DataJoint RRID:SCR_014543 DataJoint Element Session RRID:SCR_021896 DataJoint Element Array Electrophysiology RRID:SCR_021894 DataJoint Element Calcium Imaging RRID:SCR_021895 Collect summary statistics of the number of downloads and repository forking. A register for self-reporting for component adoption and use (see DataJoint Census ).","title":"Outreach Plan"},{"location":"management/outreach/#outreach-plan","text":"Broad engagement with the neuroscience community is necessary for the optimization, integration, and adoption of the Resource components. We conduct five types of outreach activities that require different approaches:","title":"Outreach Plan"},{"location":"management/outreach/#1-precursor-projects","text":"Our Selection Process requires a \"Precursor Project\" for any new experiment modality to be included in DataJoint Elements. A precursor project is a project that develops a DataJoint pipeline for its own experiments either independently or in collaboration with our team. We reach out to teams who develop DataJoint pipelines for new experiment paradigms and modalities to identify essential design motifs, analysis tools, and related tools and interfaces. We interview the core team to learn about their collaborative culture, practices, and procedures. We jointly review their open-source code and their plans for disseminating it. In many cases, our team already collaborates with such teams through our other projects and we have a good understanding of their process. As we develop a new Element to support the new modality, we remain in contact with the team to include their contribution, solicit feedback, and evaluate design tradeoffs, When the new Element is released, a full attribution is given to the Precursor Project. Rationale: The Resource does not aim to develop fundamentally new solutions for neurophysiology data acquisition and analysis. Rather it aims to systematize and disseminate existing open-source tools proven in leading research projects.","title":"1. Precursor Projects"},{"location":"management/outreach/#2-tool-developers","text":"DataJoint pipelines rely on analysis tools, atlases, data standards, archives and catalogs, and other neuroinformatics resources developed and maintained by the broader scientific community. To ensure sustainability of the Resource, we reach out to the tool developer to establish joint sustainability roadmaps.","title":"2. Tool Developers"},{"location":"management/outreach/#3-validation-sites","text":"Before new Resource components are released as part of this Resource, our team engages other research groups to evaluate these components in their projects in realistic scenarios experiments. During the Alpha testing period, the validation sites set up the workflow by either integrating Resource components with their existing workflow or by combining only the Resource components with the help of our team. We evaluate the feedback and upgrade our Resource components accordingly during this process.","title":"3. Validation Sites"},{"location":"management/outreach/#4-dissemination","text":"We conduct activities to disseminate Resource components for adoption in diverse neuroscience labs. These activities include A central website for the Resource. Presently, this site serves this purpose. Conference talks, presentations, and workshops Publications in peer-reviewed journals White papers posted on internet resources and websites On-site workshops by invitation Remote workshops and webinars Online interactive tutorials hosted on DataJoint CodeBook .","title":"4. Dissemination"},{"location":"management/outreach/#5-census","text":"In order to measure the effectiveness of the Resource, we conduct several activities to estimate the adoption and use of the Resource components: A citation mechanism for individual components of the Resource. Resource RRID DataJoint RRID:SCR_014543 DataJoint Element Session RRID:SCR_021896 DataJoint Element Array Electrophysiology RRID:SCR_021894 DataJoint Element Calcium Imaging RRID:SCR_021895 Collect summary statistics of the number of downloads and repository forking. A register for self-reporting for component adoption and use (see DataJoint Census ).","title":"5. Census"},{"location":"management/plan/","text":"Management Plan DataJoint Elements has established a Resource Management Plan to select projects for development, to assure quality, and to disseminate its output as summarized in the figure below: The following sections provide detailed information. Team Project Governance Project Selection Process Quality Assurance Contribution Guideline Outreach Plan Licenses and User Agreements","title":"Management Plan"},{"location":"management/plan/#management-plan","text":"DataJoint Elements has established a Resource Management Plan to select projects for development, to assure quality, and to disseminate its output as summarized in the figure below: The following sections provide detailed information. Team Project Governance Project Selection Process Quality Assurance Contribution Guideline Outreach Plan Licenses and User Agreements","title":"Management Plan"},{"location":"management/quality-assurance/","text":"Quality Assurance DataJoint and DataJoint Elements serve as a framework and starting points for numerous new projects, setting the standard of quality for data architecture and software design. To ensure higher quality, the following policies have been adopted into the software development lifecycle (SDLC). Coding Standards When writing code, the following principles should be observed. Style : Code shall be written for clear readability. Uniform and clear naming conventions, module structure, and formatting requirements shall be established across all components of the project. Python's PEP8 standard offers clear guidance to this regard which can similarly be applied to all languages. Maintenance Overhead : Code base size should be noted to prevent large, unnecessarily complex solutions from being introduced. The idea is that the larger the code base, the more there is to review and maintain. Therefore, we should aim to find a compromise where we can keep the code base from becoming too large without adding convoluted complexity. Performance : Performance drawbacks should be avoided, controlled, or, at least, be properly monitored and justified. For instance: memory management, garbage collection, disk reads/writes, and processing overhead should be regarded to ensure that an efficient solution is achieved. Automated Testing All components and their revisions must include appropriate automated software testing to be considered for release. The core framework must undergo thorough performance evaluation and comprehensive integration testing. Generally, this includes tests related to: Syntax : Verify that the code base does not contain any syntax errors and will run or compile successfully. Unit & Integration : Verify that low-level, method-specific tests (unit tests) and any tests related coordinated interface between methods (integration tests) pass successfully. Typically, when bugs are patched or features are introduced, unit and integration tests are added to ensure that the use-case intended to be satisfied is accounted for. This helps us prevent any regression in functionality. Style : Verify that the code base adheres to style guides for optimal readability. Code Coverage : Verify that the code base has similar or better code coverage than the last run. Code Reviews When introducing new code to the code base, the following will be required for acceptance by DataJoint core team into the main code repository. Independence : Proposed changes should not directly alter the code base in the review process. New changes should be applied separately on a copy of the code base and proposed for review by the DataJoint core team. For example, apply changes on a GitHub fork and open a pull request targeting the main branch once ready for review. Etiquette : An author who has requested for a code for review should not accept and merge their own code to the code base. A reviewer should not commit any suggestions directly to the authors proposed changes but rather should allow the author to review. Coding Standards : Ensure the above coding standards are respected. Summary : A description should be included that summarizes and highlights the notable changes that are being proposed. Issue Reference : Any bugs or feature requests that have been filed in the issue tracker that would be resolved by acceptance should be properly linked and referenced. Satisfy Automated Tests : All automated tests associated with the project will be verified to be successful prior to acceptance. Documentation : Documentation should be included to reflect any new feature or behavior introduced. Release Notes : Include necessary updates to the release notes or change log to capture a summary of the patched bugs and new feature introduction. Proper linking should be maintained to associated tickets in issue tracker and reviews. Alpha Release Process For the workflows and their revisions, the initial development and internal testing, the code will be released in Alpha . During this phase, we will engage external research teams to test and validate the complete workflows in real-life experiments with our team's engineering support. During this phase, significant design changes may be performed and not all features may be completely developed. However, several features should be usable and suitable for testing and validation. Criteria to participate as validation sites The participating lab/group has an existing DataJoint pipeline for the Element(s) to be connected to The DataJoint pipeline code base is hosted as a Github repository (either public or private) The participating lab/group has existing datasets in the format supported by the Element(s) for the purpose of this validation The participating lab/group has a dedicated point of contact person to work closely with DataJoint\u00ae engineering team for this validation effort Criteria for a successful validation Able to connect the Element(s) to existing pipeline and all schemas/tables declared without errors Successful ingestion of data for at least 2 experimental sessions Inspection/verification that the data are ingested correctly by the participating lab/group - e.g. manual inspection of ingested traces, plotting of spatial footprints, spike trains, etc. to confirm correctness of ingested data Beta Release Process After the initial validation phase, we make the workflows available to the general public with a warning of Beta status and that the released code may be subject to errors and changes. During this phase, feature development should be complete with a focus on collecting user feedback to make design improvements and bug fixes. Official Release Process After gaining confidence of user satisfaction by resolving concerns raised in Beta , the workflows are declared officially released and announced to the community. Maintenance Support Lifecycle Revision of the workflows will be released with a version specification that clearly identifies whether in Alpha , Beta , or Official release status. Quality assurance process will be followed for all iterations and new designs. If the updates require changes in the design of the database schema or formats, a process for data migration will be provided. User Feedback & Issue Tracking All components will be organized in GitHub repositories with guidelines for contribution, feedback, and issue submission to the issue tracker. For more information on the general policy around issue filing, tracking, and escalation, see the DataJoint Open-Source Contribute policy. Typically issues will be prioritized based on their criticality and impact. If new feature requirements become apparent, this may trigger the creation of a separate workflow or a major revision of an existing workflow.","title":"Quality Assurance"},{"location":"management/quality-assurance/#quality-assurance","text":"DataJoint and DataJoint Elements serve as a framework and starting points for numerous new projects, setting the standard of quality for data architecture and software design. To ensure higher quality, the following policies have been adopted into the software development lifecycle (SDLC).","title":"Quality Assurance"},{"location":"management/quality-assurance/#coding-standards","text":"When writing code, the following principles should be observed. Style : Code shall be written for clear readability. Uniform and clear naming conventions, module structure, and formatting requirements shall be established across all components of the project. Python's PEP8 standard offers clear guidance to this regard which can similarly be applied to all languages. Maintenance Overhead : Code base size should be noted to prevent large, unnecessarily complex solutions from being introduced. The idea is that the larger the code base, the more there is to review and maintain. Therefore, we should aim to find a compromise where we can keep the code base from becoming too large without adding convoluted complexity. Performance : Performance drawbacks should be avoided, controlled, or, at least, be properly monitored and justified. For instance: memory management, garbage collection, disk reads/writes, and processing overhead should be regarded to ensure that an efficient solution is achieved.","title":"Coding Standards"},{"location":"management/quality-assurance/#automated-testing","text":"All components and their revisions must include appropriate automated software testing to be considered for release. The core framework must undergo thorough performance evaluation and comprehensive integration testing. Generally, this includes tests related to: Syntax : Verify that the code base does not contain any syntax errors and will run or compile successfully. Unit & Integration : Verify that low-level, method-specific tests (unit tests) and any tests related coordinated interface between methods (integration tests) pass successfully. Typically, when bugs are patched or features are introduced, unit and integration tests are added to ensure that the use-case intended to be satisfied is accounted for. This helps us prevent any regression in functionality. Style : Verify that the code base adheres to style guides for optimal readability. Code Coverage : Verify that the code base has similar or better code coverage than the last run.","title":"Automated Testing"},{"location":"management/quality-assurance/#code-reviews","text":"When introducing new code to the code base, the following will be required for acceptance by DataJoint core team into the main code repository. Independence : Proposed changes should not directly alter the code base in the review process. New changes should be applied separately on a copy of the code base and proposed for review by the DataJoint core team. For example, apply changes on a GitHub fork and open a pull request targeting the main branch once ready for review. Etiquette : An author who has requested for a code for review should not accept and merge their own code to the code base. A reviewer should not commit any suggestions directly to the authors proposed changes but rather should allow the author to review. Coding Standards : Ensure the above coding standards are respected. Summary : A description should be included that summarizes and highlights the notable changes that are being proposed. Issue Reference : Any bugs or feature requests that have been filed in the issue tracker that would be resolved by acceptance should be properly linked and referenced. Satisfy Automated Tests : All automated tests associated with the project will be verified to be successful prior to acceptance. Documentation : Documentation should be included to reflect any new feature or behavior introduced. Release Notes : Include necessary updates to the release notes or change log to capture a summary of the patched bugs and new feature introduction. Proper linking should be maintained to associated tickets in issue tracker and reviews.","title":"Code Reviews"},{"location":"management/quality-assurance/#alpha-release-process","text":"For the workflows and their revisions, the initial development and internal testing, the code will be released in Alpha . During this phase, we will engage external research teams to test and validate the complete workflows in real-life experiments with our team's engineering support. During this phase, significant design changes may be performed and not all features may be completely developed. However, several features should be usable and suitable for testing and validation.","title":"Alpha Release Process"},{"location":"management/quality-assurance/#criteria-to-participate-as-validation-sites","text":"The participating lab/group has an existing DataJoint pipeline for the Element(s) to be connected to The DataJoint pipeline code base is hosted as a Github repository (either public or private) The participating lab/group has existing datasets in the format supported by the Element(s) for the purpose of this validation The participating lab/group has a dedicated point of contact person to work closely with DataJoint\u00ae engineering team for this validation effort","title":"Criteria to participate as validation sites"},{"location":"management/quality-assurance/#criteria-for-a-successful-validation","text":"Able to connect the Element(s) to existing pipeline and all schemas/tables declared without errors Successful ingestion of data for at least 2 experimental sessions Inspection/verification that the data are ingested correctly by the participating lab/group - e.g. manual inspection of ingested traces, plotting of spatial footprints, spike trains, etc. to confirm correctness of ingested data","title":"Criteria for a successful validation"},{"location":"management/quality-assurance/#beta-release-process","text":"After the initial validation phase, we make the workflows available to the general public with a warning of Beta status and that the released code may be subject to errors and changes. During this phase, feature development should be complete with a focus on collecting user feedback to make design improvements and bug fixes.","title":"Beta Release Process"},{"location":"management/quality-assurance/#official-release-process","text":"After gaining confidence of user satisfaction by resolving concerns raised in Beta , the workflows are declared officially released and announced to the community.","title":"Official Release Process"},{"location":"management/quality-assurance/#maintenance-support-lifecycle","text":"Revision of the workflows will be released with a version specification that clearly identifies whether in Alpha , Beta , or Official release status. Quality assurance process will be followed for all iterations and new designs. If the updates require changes in the design of the database schema or formats, a process for data migration will be provided.","title":"Maintenance Support Lifecycle"},{"location":"management/quality-assurance/#user-feedback-issue-tracking","text":"All components will be organized in GitHub repositories with guidelines for contribution, feedback, and issue submission to the issue tracker. For more information on the general policy around issue filing, tracking, and escalation, see the DataJoint Open-Source Contribute policy. Typically issues will be prioritized based on their criticality and impact. If new feature requirements become apparent, this may trigger the creation of a separate workflow or a major revision of an existing workflow.","title":"User Feedback &amp; Issue Tracking"},{"location":"management/selection/","text":"Project Selection Process The project milestones are set annually by the team under the stewardship of the NIH programmatic staff and with the guidance of the project's Scientific Steering Group We have adopted the following general criteria for selecting and accepting new projects to be included in the Resource. Open Precursor Project(s) At least one open-source DataJoint-based precursor project must exist for any new experiment modality to be accepted for support as part of the Resource. The precursor project team must be open to interviews to describe in detail their process for the experiment workflow, tools, and interfaces. The precursor projects must provide sample data for testing during development and for tutorials. The precursor projects will be acknowledged in the development of the component. Rationale: This Resource does not aim to develop fundamentally new solutions for neurophysiology data acquisition and analysis. Rather it seeks to systematize and disseminate existing open-source tools proven in leading research projects. Impact New components proposed for support in the project must be shown to be in demand by a substantial population or research groups, on the order of 100+ labs globally. Validation Roadmap Several new research groups must express interest in becoming the validation sites during the alpha and beta release phases of the project. Our team will work closely with the validation sites to test the components in their workflow and collect feedback during the process. Sustainability For all third-party tools or resources included in the proposed component, their long-term maintenance roadmap must be established. When possible, we will contact the developer team and work with them to establish a sustainability roadmap. If no such roadmap can be established, alternative tools and resources must be identified as replacement.","title":"Project Selection Process"},{"location":"management/selection/#project-selection-process","text":"The project milestones are set annually by the team under the stewardship of the NIH programmatic staff and with the guidance of the project's Scientific Steering Group We have adopted the following general criteria for selecting and accepting new projects to be included in the Resource. Open Precursor Project(s) At least one open-source DataJoint-based precursor project must exist for any new experiment modality to be accepted for support as part of the Resource. The precursor project team must be open to interviews to describe in detail their process for the experiment workflow, tools, and interfaces. The precursor projects must provide sample data for testing during development and for tutorials. The precursor projects will be acknowledged in the development of the component. Rationale: This Resource does not aim to develop fundamentally new solutions for neurophysiology data acquisition and analysis. Rather it seeks to systematize and disseminate existing open-source tools proven in leading research projects. Impact New components proposed for support in the project must be shown to be in demand by a substantial population or research groups, on the order of 100+ labs globally. Validation Roadmap Several new research groups must express interest in becoming the validation sites during the alpha and beta release phases of the project. Our team will work closely with the validation sites to test the components in their workflow and collect feedback during the process. Sustainability For all third-party tools or resources included in the proposed component, their long-term maintenance roadmap must be established. When possible, we will contact the developer team and work with them to establish a sustainability roadmap. If no such roadmap can be established, alternative tools and resources must be identified as replacement.","title":"Project Selection Process"},{"location":"management/team/","text":"Team The project is performed by DataJoint with Dimitri Yatsenko as Principal Investigator. Scientists Dimitri Yatsenko - PI Thinh Nguyen - Data Scientist Kabilar Gunalan - Data Scientist Joseph Burling - Data Scientist Chris Brozdowski - Data Scientist Tolga Dincer - Data Scientist Engineers Raphael Guzman - Software Engineer Drew Yang - Data Systems Engineer Jeroen Verswijver - Software Engineer Carlos Ortiz - Software Engineer Adib Baji - Software Engineer Past contributors Edgar Y. Walker - System architect, Data Scientist, Project Manager (from project start to Jan, 2021) Andreas S. Tolias - Grant proposal contributor Jacob Reimer - Grant proposal contributor Shan Shen - Data Scientist Maho Sasaki - Software Engineer Daniel Sitonic - Software Engineer Christopher Turner - Data Systems Engineer David Godinez - Data Engineer Geetika Singh - Data Engineer The first-person pronouns \"we\" and \"our\" in these documents refer to the Performer Team. External contributors The principal components of the Resource are developed and distributed as open-source projects and external contributions are welcome. We have adopted the following Contribution Guide for DataJoint, DataJoint Elements, and related open-source tools: https://docs.datajoint.io/python/community/02-Contribute.html","title":"Team"},{"location":"management/team/#team","text":"The project is performed by DataJoint with Dimitri Yatsenko as Principal Investigator.","title":"Team"},{"location":"management/team/#scientists","text":"Dimitri Yatsenko - PI Thinh Nguyen - Data Scientist Kabilar Gunalan - Data Scientist Joseph Burling - Data Scientist Chris Brozdowski - Data Scientist Tolga Dincer - Data Scientist","title":"Scientists"},{"location":"management/team/#engineers","text":"Raphael Guzman - Software Engineer Drew Yang - Data Systems Engineer Jeroen Verswijver - Software Engineer Carlos Ortiz - Software Engineer Adib Baji - Software Engineer","title":"Engineers"},{"location":"management/team/#past-contributors","text":"Edgar Y. Walker - System architect, Data Scientist, Project Manager (from project start to Jan, 2021) Andreas S. Tolias - Grant proposal contributor Jacob Reimer - Grant proposal contributor Shan Shen - Data Scientist Maho Sasaki - Software Engineer Daniel Sitonic - Software Engineer Christopher Turner - Data Systems Engineer David Godinez - Data Engineer Geetika Singh - Data Engineer The first-person pronouns \"we\" and \"our\" in these documents refer to the Performer Team.","title":"Past contributors"},{"location":"management/team/#external-contributors","text":"The principal components of the Resource are developed and distributed as open-source projects and external contributions are welcome. We have adopted the following Contribution Guide for DataJoint, DataJoint Elements, and related open-source tools: https://docs.datajoint.io/python/community/02-Contribute.html","title":"External contributors"},{"location":"usage/adopt/","text":"Adopt You have several options for adopting DataJoint workflows for your own experiments. Adopt independently DataJoint Elements are designed for adoption by independent users with moderate software development skills, good understanding of DataJoint principles, and adequate IT expertise or support. If you have not yet used DataJoint, we recommend completing our online training tutorials or attending a workshop either online or in person. Tutorials: https://codebook.datajoint.io https://tutorials.datajoint.io https://docs.datajoint.org Support from DataJoint Our team provides support to labs to adopt DataJoint workflows in their research. This includes: User training Developer training Data and computation hosting on your premises using your own cloud accounts fully managed cloud hosting by DataJoint Workflow execution configuration and automation optional fully managed service by DataJoint; Interfaces for data entry, export and publishing During alpha and beta testing phases, these services may be subsidized by the grant funding for qualified research groups.","title":"Adopt"},{"location":"usage/adopt/#adopt","text":"You have several options for adopting DataJoint workflows for your own experiments.","title":"Adopt"},{"location":"usage/adopt/#adopt-independently","text":"DataJoint Elements are designed for adoption by independent users with moderate software development skills, good understanding of DataJoint principles, and adequate IT expertise or support. If you have not yet used DataJoint, we recommend completing our online training tutorials or attending a workshop either online or in person. Tutorials: https://codebook.datajoint.io https://tutorials.datajoint.io https://docs.datajoint.org","title":"Adopt independently"},{"location":"usage/adopt/#support-from-datajoint","text":"Our team provides support to labs to adopt DataJoint workflows in their research. This includes: User training Developer training Data and computation hosting on your premises using your own cloud accounts fully managed cloud hosting by DataJoint Workflow execution configuration and automation optional fully managed service by DataJoint; Interfaces for data entry, export and publishing During alpha and beta testing phases, these services may be subsidized by the grant funding for qualified research groups.","title":"Support from DataJoint"},{"location":"usage/design-principles/","text":"Design Principles The following conventions describe the Python implementation. Matlab conventions are similar and will be described separately. DataJoint Schemas DataJoint allows creating database schemas , which are namespaces for collections of related tables. The following commands declare a new schema and create the object named schema to reference the database schema. import datajoint as dj schema = dj.schema('<schema_name>') We follow the convention of having only one schema defined per Python module. Then such a module becomes a \"DataJoint schema\" comprising a python module with a corresponding database schema. The module's schema object is then used as the decorator for classes that define tables in the database. Elements An Element is a software package defining one or more DataJoint schemas serving a particular purpose. By convention, such packages are hosted in individual GitHub repositories. For example, Element element_calcium_imaging is hosted at https://github.com/datajoint/element-calcium-imaging , and contains two DataJoint schemas: scan and imaging . Deferred schemas A deferred schema is one in which the name of the database schema name is not specified. This module does not declare schema and tables upon import. Instead, they are declared by calling schema.activate('<schema_name>') after import. By convention, all modules corresponding to deferred schema must declare the function activate which in turn calls schema.activate . Thus Element modules begin with import datajoint as dj schema = dj.schema() def activate(schema_name): schema.activate(schema_name) However, many activate functions perform other work associated with activating the schema such as activating other schemas upstream. Linking Module To make the code more modular with fewer dependencies, Elements' modules do not import upstream schemas directly. Instead, all required classes and functions must be defined in a linking_module and passed to the module's activate function. For instance, the element_calcium_imaging.scan module receives its required functions from the linking module passed into the module's activate function. See the corresponding workflow for an example of how the linking module is passed into the Element's module.","title":"Design Principles"},{"location":"usage/design-principles/#design-principles","text":"The following conventions describe the Python implementation. Matlab conventions are similar and will be described separately.","title":"Design Principles"},{"location":"usage/design-principles/#datajoint-schemas","text":"DataJoint allows creating database schemas , which are namespaces for collections of related tables. The following commands declare a new schema and create the object named schema to reference the database schema. import datajoint as dj schema = dj.schema('<schema_name>') We follow the convention of having only one schema defined per Python module. Then such a module becomes a \"DataJoint schema\" comprising a python module with a corresponding database schema. The module's schema object is then used as the decorator for classes that define tables in the database.","title":"DataJoint Schemas"},{"location":"usage/design-principles/#elements","text":"An Element is a software package defining one or more DataJoint schemas serving a particular purpose. By convention, such packages are hosted in individual GitHub repositories. For example, Element element_calcium_imaging is hosted at https://github.com/datajoint/element-calcium-imaging , and contains two DataJoint schemas: scan and imaging .","title":"Elements"},{"location":"usage/design-principles/#deferred-schemas","text":"A deferred schema is one in which the name of the database schema name is not specified. This module does not declare schema and tables upon import. Instead, they are declared by calling schema.activate('<schema_name>') after import. By convention, all modules corresponding to deferred schema must declare the function activate which in turn calls schema.activate . Thus Element modules begin with import datajoint as dj schema = dj.schema() def activate(schema_name): schema.activate(schema_name) However, many activate functions perform other work associated with activating the schema such as activating other schemas upstream.","title":"Deferred schemas"},{"location":"usage/design-principles/#linking-module","text":"To make the code more modular with fewer dependencies, Elements' modules do not import upstream schemas directly. Instead, all required classes and functions must be defined in a linking_module and passed to the module's activate function. For instance, the element_calcium_imaging.scan module receives its required functions from the linking module passed into the module's activate function. See the corresponding workflow for an example of how the linking module is passed into the Element's module.","title":"Linking Module"},{"location":"usage/glossary/","text":"Glossary The following are some terms used in the Resource. DataJoint a software framework for database programming directly from matlab and python. Thanks to its support of automated computational dependencies, DataJoint serves as a workflow management system. DataJoint Workflow, Experiment Workflow, or simply Workflow a formal representation of the steps for executing an experiment from data collection to analysis. Also the software configured for performing these steps. A typical workflow is composed of tables with inter-dependencies and processes to compute and insert data into the tables. DataJoint Pipeline the data schemas and transformations underlying a DataJoint workflow. DataJoint allows defining code that specifies both the workflow and the data pipeline, and we have used the words \"pipeline\" and \"workflow\" almost interchangeably. DataJoint Schema a software module implementing a portion of an experiment workflow. Includes database table definitions, dependencies, and associated computations. DataJoint Elements software modules implementing portions of experiment workflows designed for ease of integration into diverse custom workflows. djHub our team's internal platform for delivering cloud-based infrastructure to support online training resources, validation studies, and collaborative projects.","title":"Glossary"},{"location":"usage/glossary/#glossary","text":"The following are some terms used in the Resource. DataJoint a software framework for database programming directly from matlab and python. Thanks to its support of automated computational dependencies, DataJoint serves as a workflow management system. DataJoint Workflow, Experiment Workflow, or simply Workflow a formal representation of the steps for executing an experiment from data collection to analysis. Also the software configured for performing these steps. A typical workflow is composed of tables with inter-dependencies and processes to compute and insert data into the tables. DataJoint Pipeline the data schemas and transformations underlying a DataJoint workflow. DataJoint allows defining code that specifies both the workflow and the data pipeline, and we have used the words \"pipeline\" and \"workflow\" almost interchangeably. DataJoint Schema a software module implementing a portion of an experiment workflow. Includes database table definitions, dependencies, and associated computations. DataJoint Elements software modules implementing portions of experiment workflows designed for ease of integration into diverse custom workflows. djHub our team's internal platform for delivering cloud-based infrastructure to support online training resources, validation studies, and collaborative projects.","title":"Glossary"},{"location":"usage/install/","text":"Setup instructions for workflows created with the DataJoint Elements The following document describes the steps to setup a development environment so that you can use the DataJoint Elements to build and run a workflow on your local machine. The DataJoint Elements can be combined together to create a workflow that matches your experimental setup. We have created example workflows (e.g. workflow-array-ephys , workflow-calcium-imaging ) for your reference. In this tutorial we will install these example DataJoint workflows. These instructions can be adapted for your custom DataJoint workflow. There are several ways to create a development environment. Here we will discuss one method in detail, and will highlight other methods along the way. If you have already set up certain components, feel free to skip those sections. You will need administrative privileges on your system for the following setup instructions. System architecture The above diagram describes the general components for a local DataJoint development environment. Install an integrated development environment DataJoint development and use can be done with a plain text editor in the terminal. However, an integrated development environment (IDE) can improve your experience. Several IDEs are available. In this setup example, we will use Microsoft's Visual Studio Code. Installation instructions here Install a relational database A key feature of DataJoint is the ability to connect with a database server from a scientific programming environment (i.e., Python or MATLAB) so that your experimental data can be stored in the database and downloaded from the database. There are several options if you would like to install a local relational database server + Docker image for MySQL server configured for use with DataJoint + [Install MariaDB server](https://mariadb.com/kb/en/binary-packages/) Alternatively, for simplicity of this tutorial you can use the DataJoint Playground tutorial database located at tutorial-db.datajoint.io which has already been configured. Please note that the tutorial database should not be used for your experimental analysis as the storage is not persistent. Install a version control system Git is an open-source, distributed version control system for collaborating with software development. GitHub is a platform that hosts projects managed with Git. As the example DataJoint workflows are hosted on GitHub, we will use Git to clone (i.e., download) this repository. For your own DataJoint workflow development we recommended that you use Git and GitHub for collaboration. Many systems come preinstalled with Git. You can test if Git is already installed by typing git in a terminal window. If Git is not installed on your system, Install Git . Install a virtual environment A virtual environment allows you to install the packages required for a specific project within an isolated environment on your computer. It is highly recommended (though not strictly required) to create a virtual environment to run the workflow. Conda and virtualenv are virtual environment managers and you can use either option. Below you will find instructions for conda. Miniconda is a minimal installer for conda. Follow the installer instructions for your operating system. You may need to add the Miniconda directory to the PATH environment variable First locate the Miniconda directory Then modify and run the following command bash export PATH=\"<absolute-path-to-miniconda-directory>/bin:$PATH\" Create a new conda environment bash conda create -n <environment_name> python=<version> Example command to create a conda environment bash conda create -n workflow-array-ephys python=3.8.11 Activate the conda environment bash conda activate <environment_name> Install Jupyter Notebook packages Install the following, if you are using Jupyter Notebook. bash conda install jupyter ipykernel nb_conda_kernels Install the following, for dj.Diagram to render. bash conda install graphviz python-graphviz pydotplus Clone and install the relevant repository In a terminal window and change the directory to where you want to clone the repository bash cd ~/Projects Clone the relevant repository, often one of the workflows bash git clone https://github.com/datajoint/<repository> Change into the cloned directory bash cd <repository> From the root of the cloned repository directory. Note: the -e flag, which will will install this repository in editable mode, in case there's a need to modify the code (e.g. the workflow pipeline.py or paths.py scripts). If no such modification is required, using pip install . is sufficient. bash pip install -e . Install element-interface , which contains scripts to load data for many of our Elements, and all workflows bash pip install \"element-interface @ git+https://github.com/datajoint/element-interface\" Items specific to workflow-calcium-imaging Click to expand details element-interface can also be used to install packages used for reading acquired data (e.g., scanreader ) and running analyses (e.g., CaImAn ). Install element-interface with scanreader bash pip install \"element-interface[scanreader] @ git+https://github.com/datajoint/element-interface\" Install element-interface with sbxreader bash pip install \"element-interface[sbxreader] @ git+https://github.com/datajoint/element-interface\" Install element-interface with Suite2p bash pip install \"element-interface[suite2p] @ git+https://github.com/datajoint/element-interface\" Install element-interface with CaImAn requires two separate commands bash pip install \"element-interface[caiman_requirements] @ git+https://github.com/datajoint/element-interface\" pip install \"element-interface[caiman] @ git+https://github.com/datajoint/element-interface\" Example element-interface installation with multiple packages bash pip install \"element-interface[caiman_requirements] @ git+https://github.com/datajoint/element-interface\" pip install \"element-interface[scanreader,sbxreader,suite2p,caiman] @ git+https://github.com/datajoint/element-interface\" Set up a connection to the database server One way to set up a connection to the database server with DataJoint is to create a local configuration file (i.e., dj_local_conf.json ) at the root of the repository folder, with the following template: json { \"database.host\": \"<hostname>\", \"database.user\": \"<username>\", \"database.password\": \"<password>\", \"loglevel\": \"INFO\", \"safemode\": true, \"display.limit\": 7, \"display.width\": 14, \"display.show_tuple_count\": true, \"custom\": { \"database.prefix\": \"<username_>\" } } Specify the database's hostname , username , and password . If using the Docker image for MySQL server configured for use with DataJoint then the hostname will be localhost . If using the tutorial database, the hostname will be tutorial-db.datajoint.io . And the username and password will be the credentials for your DataJoint account . Specify a database.prefix which will be the prefix for your schema names. For a local setup, it can be set as you see fit (e.g., neuro_ ). For the tutorial-db database, you will use your DataJoint username. Specific workflows will require additional information in the custom field, including paths to data directories, following the convention described in the directory structure section . If multiple root directories exist, include all in the relevant json array. + workflow-array-ephys Click to expand ```json \"custom\": { \"database.prefix\": \"<username_>\", \"ephys_root_data_dir\": [\"Full path to root directory of raw data\", \"Full path to root directory of processed data\"] } ``` </details> + workflow-calcium-imaging Click to expand ```json \"custom\": { \"database.prefix\": \"<username_>\", \"imaging_root_data_dir\": [\"Full path to root directory of raw data\", \"Full path to root directory of processed data\"] } ``` </details> + workflow-miniscope Click to expand ```json \"custom\": { \"database.prefix\": \"<username_>\", \"miniscope_root_data_dir\": [\"Full path to root directory of raw data\", \"Full path to root directory of processed data\"] } ``` </details> + workflow-deeplabcut Click to expand ```json \"custom\": { \"database.prefix\": \"<username_>\", \"dlc_root_data_dir\": [\"Full path to root directory of raw data\", \"Full path to root directory of processed data\"] } ``` </details> Setup complete At this point the setup of this workflow is complete. Download example data We provide example data to use with the example DataJoint workflows. The data is hosted on DataJoint's Archive which is an AWS storage and can be download with djarchive-client . Install djarchive-client bash pip install git+https://github.com/datajoint/djarchive-client.git In your python interpreter, import the client python import djarchive_client client = djarchive_client.client() Browse the available datasets python list(client.datasets()) Each datasets has different versions associated with the version of the workflow package. Browse the revisions. python list(client.revisions()) Prepare a directory to store the download data, for example in /tmp bash mkdir /tmp/example_data Download a given dataset python client.download('<workflow-dataset>', target_directory='/tmp/example_data', revision='<revision>') We will use this data as an example for the tutorial notebooks for each workflow. If you want to use for own dataset for the workflow, change the path accordingly. Directory organization workflow-array-ephys Click to expand details /tmp/example_data/ - subject6 - session1 - towersTask_g0_imec0 - towersTask_g0_t0_nidq.meta - towersTask_g0_t0.nidq.bin The example subject6/session1 data was recorded with SpikeGLX and processed with Kilosort2. element-array-ephys and workflow-array-ephys also support data recorded with OpenEphys. workflow-calcium-imaging Click to expand details /tmp/example_data/ - subject3/ - 210107_run00_orientation_8dir/ - run00_orientation_8dir_000_000.sbx - run00_orientation_8dir_000_000.mat - suite2p/ - combined - plane0 - plane1 - plane2 - plane3 - subject7/ - session1 - suite2p - plane0 The example subject3 data was recorded with Scanbox and processed with Suite2p. The example subject7 data was recorded with ScanImage and processed with Suite2p. element-calcium-imaging and workflow-calcium-imaging also support data processed with CaImAn. Directory structure and file naming convention The workflow presented here is designed to work with the directory structure and file naming convention as described below. workflow-array-ephys Click to expand details The ephys_root_data_dir is configurable in the dj_local_conf.json , under custom/ephys_root_data_dir variable. The subject directory names must match the identifiers of your subjects in the subjects.csv script ( ./user_data/subjects.csv ). The session directories can have any naming convention. Each session can have multiple probes, the probe directories must match the following naming convention: `*[0-9]` (where `[0-9]` is a one digit number specifying the probe number) Each probe directory should contain: One neuropixels meta file, with the following naming convention: *[0-9].ap.meta Potentially one Kilosort output folder <ephys_root_data_dir>/ \u2514\u2500\u2500\u2500<subject1>/ # Subject name in `subjects.csv` \u2502 \u2514\u2500\u2500\u2500<session0>/ # Session directory in `sessions.csv` \u2502 \u2502 \u2514\u2500\u2500\u2500imec0/ \u2502 \u2502 \u2502 \u2502 *imec0.ap.meta \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500ksdir/ \u2502 \u2502 \u2502 \u2502 spike_times.npy \u2502 \u2502 \u2502 \u2502 templates.npy \u2502 \u2502 \u2502 \u2502 ... \u2502 \u2502 \u2514\u2500\u2500\u2500imec1/ \u2502 \u2502 \u2502 *imec1.ap.meta \u2502 \u2502 \u2514\u2500\u2500\u2500ksdir/ \u2502 \u2502 \u2502 spike_times.npy \u2502 \u2502 \u2502 templates.npy \u2502 \u2502 \u2502 ... \u2502 \u2514\u2500\u2500\u2500<session1>/ \u2502 \u2502 \u2502 ... \u2514\u2500\u2500\u2500<subject2>/ \u2502 \u2502 ... workflow-calcium-imaging Click to expand details Note: the element-calcium-imaging is designed to accommodate multiple scans per session, however, in this particular workflow-calcium-imaging , we take the assumption that there is only one scan per session. The imaging_root_data_dir directory is configurable in the dj_local_conf.json , under the custom/imaging_root_data_dir variable The subject directory names must match the identifiers of your subjects in the subjects.csv script ( ./user_data/subjects.csv ). The session directories can have any naming convention Each session directory should contain: All .tif or .sbx files for the scan, with any naming convention One suite2p subfolder per session folder, containing the Suite2p analysis outputs One caiman subfolder per session folder, containing the CaImAn analysis output .hdf5 file, with any naming convention imaging_root_data_dir/ \u2514\u2500\u2500\u2500<subject1>/ # Subject name in `subjects.csv` \u2502 \u2514\u2500\u2500\u2500<session0>/ # Session directory in `sessions.csv` \u2502 \u2502 \u2502 scan_0001.tif \u2502 \u2502 \u2502 scan_0002.tif \u2502 \u2502 \u2502 scan_0003.tif \u2502 \u2502 \u2502 ... \u2502 \u2502 \u2514\u2500\u2500\u2500suite2p/ \u2502 \u2502 \u2502 ops1.npy \u2502 \u2502 \u2514\u2500\u2500\u2500plane0/ \u2502 \u2502 \u2502 \u2502 ops.npy \u2502 \u2502 \u2502 \u2502 spks.npy \u2502 \u2502 \u2502 \u2502 stat.npy \u2502 \u2502 \u2502 \u2502 ... \u2502 \u2502 \u2514\u2500\u2500\u2500plane1/ \u2502 \u2502 \u2502 ops.npy \u2502 \u2502 \u2502 spks.npy \u2502 \u2502 \u2502 stat.npy \u2502 \u2502 \u2502 ... \u2502 \u2502 \u2514\u2500\u2500\u2500caiman/ \u2502 \u2502 \u2502 analysis_results.hdf5 \u2502 \u2514\u2500\u2500\u2500<session1>/ # Session directory in `sessions.csv` \u2502 \u2502 \u2502 scan_0001.tif \u2502 \u2502 \u2502 scan_0002.tif \u2502 \u2502 \u2502 ... \u2514\u2500\u2500\u2500<subject2>/ # Subject name in `subjects.csv` \u2502 \u2502 ... Interacting with the DataJoint workflow Connect to the database and import tables python from <relevant-workflow>.pipeline import * View the declared tables - workflow-array-ephys Click to expand details ```python subject.Subject() session.Session() ephys.ProbeInsertion() ephys.EphysRecording() ephys.Clustering() ephys.Clustering.Unit() ``` - workflow-calcium-imaging Click to expand details ```python subject.Subject() session.Session() scan.Scan() scan.ScanInfo() imaging.ProcessingParamSet() imaging.ProcessingTask() ``` </details> For an in depth explanation of how to run the workflows and explore the data, please refer to the following workflow specific Jupyter notebooks. + workflow-array-ephys Jupyter notebooks + workflow-calcium-imaging Jupyter notebooks DataJoint LabBook DataJoint LabBook is a graphical user interface to facilitate working with DataJoint tables. DataJoint LabBook Documentation , including prerequisites, installation, and running the application DataJoint LabBook GitHub Repository Developer guide Development mode installation This method allows you to modify the source code for example DataJoint workflows (e.g. workflow-array-ephys , workflow-calcium-imaging ) and their dependencies (i.e., DataJoint Elements). Launch a new terminal and change directory to where you want to clone the repositories bash cd ~/Projects workflow-array-ephys Click to expand details Clone the repositories bash git clone https://github.com/datajoint/element-lab git clone https://github.com/datajoint/element-animal git clone https://github.com/datajoint/element-session git clone https://github.com/datajoint/element-interface git clone https://github.com/datajoint/element-array-ephys git clone https://github.com/datajoint/workflow-array-ephys Install each package with the -e option bash pip install -e ./element-lab pip install -e ./element-animal pip install -e ./element-session pip install -e ./element-interface pip install -e ./element-array-ephys pip install -e ./workflow-array-ephys workflow-calcium-imaging Click to expand details Clone the repositories bash git clone https://github.com/datajoint/element-lab git clone https://github.com/datajoint/element-animal git clone https://github.com/datajoint/element-session git clone https://github.com/datajoint/element-interface git clone https://github.com/datajoint/element-calcium-imaging git clone https://github.com/datajoint/workflow-calcium-imaging Install each package with the -e option bash pip install -e ./element-lab pip install -e ./element-animal pip install -e ./element-session pip install -e ./element-interface pip install -e ./element-calcium-imaging pip install -e ./workflow-calcium-imaging Optionally drop all schemas If required to drop all schemas, the following is the dependency order. Also refer to notebooks/06-drop-optional.ipynb within the respective workflow . workflow-array-ephys Click to expand details ``` from workflow_array_ephys.pipeline import * ephys.schema.drop() probe.schema.drop() session.schema.drop() subject.schema.drop() lab.schema.drop() ``` workflow-calcium-imaging Click to expand details ``` from workflow_calcium_imaging.pipeline import * imaging.schema.drop() scan.schema.drop() session.schema.drop() subject.schema.drop() lab.schema.drop() ``` workflow-miniscope Click to expand details ``` from workflow_miniscope.pipeline import * miniscope.schema.drop() session.schema.drop() subject.schema.drop() lab.schema.drop() ``` Run integration tests Download the test dataset to your local machine. Note the directory where the dataset is saved (e.g. /tmp/testset ). Create an .env file within the docker directory with the following content. Replace /tmp/testset with the directory where you have the test dataset downloaded. TEST_DATA_DIR=/tmp/testset If testing an unreleased version of the element or your fork of an element or the workflow , within the Dockerfile uncomment the lines from the different options presented. This will allow you to install the repositories of interest and run the integration tests on those packages. Be sure that the element package version matches the version in the requirements.txt of the workflow . Run the Docker container. docker-compose -f ./docker/docker-compose-test.yaml up --build","title":"Setup"},{"location":"usage/install/#setup-instructions-for-workflows-created-with-the-datajoint-elements","text":"The following document describes the steps to setup a development environment so that you can use the DataJoint Elements to build and run a workflow on your local machine. The DataJoint Elements can be combined together to create a workflow that matches your experimental setup. We have created example workflows (e.g. workflow-array-ephys , workflow-calcium-imaging ) for your reference. In this tutorial we will install these example DataJoint workflows. These instructions can be adapted for your custom DataJoint workflow. There are several ways to create a development environment. Here we will discuss one method in detail, and will highlight other methods along the way. If you have already set up certain components, feel free to skip those sections. You will need administrative privileges on your system for the following setup instructions.","title":"Setup instructions for workflows created with the DataJoint Elements"},{"location":"usage/install/#system-architecture","text":"The above diagram describes the general components for a local DataJoint development environment.","title":"System architecture"},{"location":"usage/install/#install-an-integrated-development-environment","text":"DataJoint development and use can be done with a plain text editor in the terminal. However, an integrated development environment (IDE) can improve your experience. Several IDEs are available. In this setup example, we will use Microsoft's Visual Studio Code. Installation instructions here","title":"Install an integrated development environment"},{"location":"usage/install/#install-a-relational-database","text":"A key feature of DataJoint is the ability to connect with a database server from a scientific programming environment (i.e., Python or MATLAB) so that your experimental data can be stored in the database and downloaded from the database. There are several options if you would like to install a local relational database server + Docker image for MySQL server configured for use with DataJoint + [Install MariaDB server](https://mariadb.com/kb/en/binary-packages/) Alternatively, for simplicity of this tutorial you can use the DataJoint Playground tutorial database located at tutorial-db.datajoint.io which has already been configured. Please note that the tutorial database should not be used for your experimental analysis as the storage is not persistent.","title":"Install a relational database"},{"location":"usage/install/#install-a-version-control-system","text":"Git is an open-source, distributed version control system for collaborating with software development. GitHub is a platform that hosts projects managed with Git. As the example DataJoint workflows are hosted on GitHub, we will use Git to clone (i.e., download) this repository. For your own DataJoint workflow development we recommended that you use Git and GitHub for collaboration. Many systems come preinstalled with Git. You can test if Git is already installed by typing git in a terminal window. If Git is not installed on your system, Install Git .","title":"Install a version control system"},{"location":"usage/install/#install-a-virtual-environment","text":"A virtual environment allows you to install the packages required for a specific project within an isolated environment on your computer. It is highly recommended (though not strictly required) to create a virtual environment to run the workflow. Conda and virtualenv are virtual environment managers and you can use either option. Below you will find instructions for conda. Miniconda is a minimal installer for conda. Follow the installer instructions for your operating system. You may need to add the Miniconda directory to the PATH environment variable First locate the Miniconda directory Then modify and run the following command bash export PATH=\"<absolute-path-to-miniconda-directory>/bin:$PATH\" Create a new conda environment bash conda create -n <environment_name> python=<version> Example command to create a conda environment bash conda create -n workflow-array-ephys python=3.8.11 Activate the conda environment bash conda activate <environment_name>","title":"Install a virtual environment"},{"location":"usage/install/#install-jupyter-notebook-packages","text":"Install the following, if you are using Jupyter Notebook. bash conda install jupyter ipykernel nb_conda_kernels Install the following, for dj.Diagram to render. bash conda install graphviz python-graphviz pydotplus","title":"Install Jupyter Notebook packages"},{"location":"usage/install/#clone-and-install-the-relevant-repository","text":"In a terminal window and change the directory to where you want to clone the repository bash cd ~/Projects Clone the relevant repository, often one of the workflows bash git clone https://github.com/datajoint/<repository> Change into the cloned directory bash cd <repository> From the root of the cloned repository directory. Note: the -e flag, which will will install this repository in editable mode, in case there's a need to modify the code (e.g. the workflow pipeline.py or paths.py scripts). If no such modification is required, using pip install . is sufficient. bash pip install -e . Install element-interface , which contains scripts to load data for many of our Elements, and all workflows bash pip install \"element-interface @ git+https://github.com/datajoint/element-interface\" Items specific to workflow-calcium-imaging Click to expand details element-interface can also be used to install packages used for reading acquired data (e.g., scanreader ) and running analyses (e.g., CaImAn ). Install element-interface with scanreader bash pip install \"element-interface[scanreader] @ git+https://github.com/datajoint/element-interface\" Install element-interface with sbxreader bash pip install \"element-interface[sbxreader] @ git+https://github.com/datajoint/element-interface\" Install element-interface with Suite2p bash pip install \"element-interface[suite2p] @ git+https://github.com/datajoint/element-interface\" Install element-interface with CaImAn requires two separate commands bash pip install \"element-interface[caiman_requirements] @ git+https://github.com/datajoint/element-interface\" pip install \"element-interface[caiman] @ git+https://github.com/datajoint/element-interface\" Example element-interface installation with multiple packages bash pip install \"element-interface[caiman_requirements] @ git+https://github.com/datajoint/element-interface\" pip install \"element-interface[scanreader,sbxreader,suite2p,caiman] @ git+https://github.com/datajoint/element-interface\"","title":"Clone and install the relevant repository"},{"location":"usage/install/#set-up-a-connection-to-the-database-server","text":"One way to set up a connection to the database server with DataJoint is to create a local configuration file (i.e., dj_local_conf.json ) at the root of the repository folder, with the following template: json { \"database.host\": \"<hostname>\", \"database.user\": \"<username>\", \"database.password\": \"<password>\", \"loglevel\": \"INFO\", \"safemode\": true, \"display.limit\": 7, \"display.width\": 14, \"display.show_tuple_count\": true, \"custom\": { \"database.prefix\": \"<username_>\" } } Specify the database's hostname , username , and password . If using the Docker image for MySQL server configured for use with DataJoint then the hostname will be localhost . If using the tutorial database, the hostname will be tutorial-db.datajoint.io . And the username and password will be the credentials for your DataJoint account . Specify a database.prefix which will be the prefix for your schema names. For a local setup, it can be set as you see fit (e.g., neuro_ ). For the tutorial-db database, you will use your DataJoint username. Specific workflows will require additional information in the custom field, including paths to data directories, following the convention described in the directory structure section . If multiple root directories exist, include all in the relevant json array. + workflow-array-ephys Click to expand ```json \"custom\": { \"database.prefix\": \"<username_>\", \"ephys_root_data_dir\": [\"Full path to root directory of raw data\", \"Full path to root directory of processed data\"] } ``` </details> + workflow-calcium-imaging Click to expand ```json \"custom\": { \"database.prefix\": \"<username_>\", \"imaging_root_data_dir\": [\"Full path to root directory of raw data\", \"Full path to root directory of processed data\"] } ``` </details> + workflow-miniscope Click to expand ```json \"custom\": { \"database.prefix\": \"<username_>\", \"miniscope_root_data_dir\": [\"Full path to root directory of raw data\", \"Full path to root directory of processed data\"] } ``` </details> + workflow-deeplabcut Click to expand ```json \"custom\": { \"database.prefix\": \"<username_>\", \"dlc_root_data_dir\": [\"Full path to root directory of raw data\", \"Full path to root directory of processed data\"] } ``` </details>","title":"Set up a connection to the database server"},{"location":"usage/install/#setup-complete","text":"At this point the setup of this workflow is complete.","title":"Setup complete"},{"location":"usage/install/#download-example-data","text":"We provide example data to use with the example DataJoint workflows. The data is hosted on DataJoint's Archive which is an AWS storage and can be download with djarchive-client . Install djarchive-client bash pip install git+https://github.com/datajoint/djarchive-client.git In your python interpreter, import the client python import djarchive_client client = djarchive_client.client() Browse the available datasets python list(client.datasets()) Each datasets has different versions associated with the version of the workflow package. Browse the revisions. python list(client.revisions()) Prepare a directory to store the download data, for example in /tmp bash mkdir /tmp/example_data Download a given dataset python client.download('<workflow-dataset>', target_directory='/tmp/example_data', revision='<revision>') We will use this data as an example for the tutorial notebooks for each workflow. If you want to use for own dataset for the workflow, change the path accordingly. Directory organization workflow-array-ephys Click to expand details /tmp/example_data/ - subject6 - session1 - towersTask_g0_imec0 - towersTask_g0_t0_nidq.meta - towersTask_g0_t0.nidq.bin The example subject6/session1 data was recorded with SpikeGLX and processed with Kilosort2. element-array-ephys and workflow-array-ephys also support data recorded with OpenEphys. workflow-calcium-imaging Click to expand details /tmp/example_data/ - subject3/ - 210107_run00_orientation_8dir/ - run00_orientation_8dir_000_000.sbx - run00_orientation_8dir_000_000.mat - suite2p/ - combined - plane0 - plane1 - plane2 - plane3 - subject7/ - session1 - suite2p - plane0 The example subject3 data was recorded with Scanbox and processed with Suite2p. The example subject7 data was recorded with ScanImage and processed with Suite2p. element-calcium-imaging and workflow-calcium-imaging also support data processed with CaImAn.","title":"Download example data"},{"location":"usage/install/#directory-structure-and-file-naming-convention","text":"The workflow presented here is designed to work with the directory structure and file naming convention as described below. workflow-array-ephys Click to expand details The ephys_root_data_dir is configurable in the dj_local_conf.json , under custom/ephys_root_data_dir variable. The subject directory names must match the identifiers of your subjects in the subjects.csv script ( ./user_data/subjects.csv ). The session directories can have any naming convention. Each session can have multiple probes, the probe directories must match the following naming convention: `*[0-9]` (where `[0-9]` is a one digit number specifying the probe number) Each probe directory should contain: One neuropixels meta file, with the following naming convention: *[0-9].ap.meta Potentially one Kilosort output folder <ephys_root_data_dir>/ \u2514\u2500\u2500\u2500<subject1>/ # Subject name in `subjects.csv` \u2502 \u2514\u2500\u2500\u2500<session0>/ # Session directory in `sessions.csv` \u2502 \u2502 \u2514\u2500\u2500\u2500imec0/ \u2502 \u2502 \u2502 \u2502 *imec0.ap.meta \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500ksdir/ \u2502 \u2502 \u2502 \u2502 spike_times.npy \u2502 \u2502 \u2502 \u2502 templates.npy \u2502 \u2502 \u2502 \u2502 ... \u2502 \u2502 \u2514\u2500\u2500\u2500imec1/ \u2502 \u2502 \u2502 *imec1.ap.meta \u2502 \u2502 \u2514\u2500\u2500\u2500ksdir/ \u2502 \u2502 \u2502 spike_times.npy \u2502 \u2502 \u2502 templates.npy \u2502 \u2502 \u2502 ... \u2502 \u2514\u2500\u2500\u2500<session1>/ \u2502 \u2502 \u2502 ... \u2514\u2500\u2500\u2500<subject2>/ \u2502 \u2502 ... workflow-calcium-imaging Click to expand details Note: the element-calcium-imaging is designed to accommodate multiple scans per session, however, in this particular workflow-calcium-imaging , we take the assumption that there is only one scan per session. The imaging_root_data_dir directory is configurable in the dj_local_conf.json , under the custom/imaging_root_data_dir variable The subject directory names must match the identifiers of your subjects in the subjects.csv script ( ./user_data/subjects.csv ). The session directories can have any naming convention Each session directory should contain: All .tif or .sbx files for the scan, with any naming convention One suite2p subfolder per session folder, containing the Suite2p analysis outputs One caiman subfolder per session folder, containing the CaImAn analysis output .hdf5 file, with any naming convention imaging_root_data_dir/ \u2514\u2500\u2500\u2500<subject1>/ # Subject name in `subjects.csv` \u2502 \u2514\u2500\u2500\u2500<session0>/ # Session directory in `sessions.csv` \u2502 \u2502 \u2502 scan_0001.tif \u2502 \u2502 \u2502 scan_0002.tif \u2502 \u2502 \u2502 scan_0003.tif \u2502 \u2502 \u2502 ... \u2502 \u2502 \u2514\u2500\u2500\u2500suite2p/ \u2502 \u2502 \u2502 ops1.npy \u2502 \u2502 \u2514\u2500\u2500\u2500plane0/ \u2502 \u2502 \u2502 \u2502 ops.npy \u2502 \u2502 \u2502 \u2502 spks.npy \u2502 \u2502 \u2502 \u2502 stat.npy \u2502 \u2502 \u2502 \u2502 ... \u2502 \u2502 \u2514\u2500\u2500\u2500plane1/ \u2502 \u2502 \u2502 ops.npy \u2502 \u2502 \u2502 spks.npy \u2502 \u2502 \u2502 stat.npy \u2502 \u2502 \u2502 ... \u2502 \u2502 \u2514\u2500\u2500\u2500caiman/ \u2502 \u2502 \u2502 analysis_results.hdf5 \u2502 \u2514\u2500\u2500\u2500<session1>/ # Session directory in `sessions.csv` \u2502 \u2502 \u2502 scan_0001.tif \u2502 \u2502 \u2502 scan_0002.tif \u2502 \u2502 \u2502 ... \u2514\u2500\u2500\u2500<subject2>/ # Subject name in `subjects.csv` \u2502 \u2502 ...","title":"Directory structure and file naming convention"},{"location":"usage/install/#interacting-with-the-datajoint-workflow","text":"Connect to the database and import tables python from <relevant-workflow>.pipeline import * View the declared tables - workflow-array-ephys Click to expand details ```python subject.Subject() session.Session() ephys.ProbeInsertion() ephys.EphysRecording() ephys.Clustering() ephys.Clustering.Unit() ``` - workflow-calcium-imaging Click to expand details ```python subject.Subject() session.Session() scan.Scan() scan.ScanInfo() imaging.ProcessingParamSet() imaging.ProcessingTask() ``` </details> For an in depth explanation of how to run the workflows and explore the data, please refer to the following workflow specific Jupyter notebooks. + workflow-array-ephys Jupyter notebooks + workflow-calcium-imaging Jupyter notebooks","title":"Interacting with the DataJoint workflow"},{"location":"usage/install/#datajoint-labbook","text":"DataJoint LabBook is a graphical user interface to facilitate working with DataJoint tables. DataJoint LabBook Documentation , including prerequisites, installation, and running the application DataJoint LabBook GitHub Repository","title":"DataJoint LabBook"},{"location":"usage/install/#developer-guide","text":"","title":"Developer guide"},{"location":"usage/install/#development-mode-installation","text":"This method allows you to modify the source code for example DataJoint workflows (e.g. workflow-array-ephys , workflow-calcium-imaging ) and their dependencies (i.e., DataJoint Elements). Launch a new terminal and change directory to where you want to clone the repositories bash cd ~/Projects workflow-array-ephys Click to expand details Clone the repositories bash git clone https://github.com/datajoint/element-lab git clone https://github.com/datajoint/element-animal git clone https://github.com/datajoint/element-session git clone https://github.com/datajoint/element-interface git clone https://github.com/datajoint/element-array-ephys git clone https://github.com/datajoint/workflow-array-ephys Install each package with the -e option bash pip install -e ./element-lab pip install -e ./element-animal pip install -e ./element-session pip install -e ./element-interface pip install -e ./element-array-ephys pip install -e ./workflow-array-ephys workflow-calcium-imaging Click to expand details Clone the repositories bash git clone https://github.com/datajoint/element-lab git clone https://github.com/datajoint/element-animal git clone https://github.com/datajoint/element-session git clone https://github.com/datajoint/element-interface git clone https://github.com/datajoint/element-calcium-imaging git clone https://github.com/datajoint/workflow-calcium-imaging Install each package with the -e option bash pip install -e ./element-lab pip install -e ./element-animal pip install -e ./element-session pip install -e ./element-interface pip install -e ./element-calcium-imaging pip install -e ./workflow-calcium-imaging","title":"Development mode installation"},{"location":"usage/install/#optionally-drop-all-schemas","text":"If required to drop all schemas, the following is the dependency order. Also refer to notebooks/06-drop-optional.ipynb within the respective workflow . workflow-array-ephys Click to expand details ``` from workflow_array_ephys.pipeline import * ephys.schema.drop() probe.schema.drop() session.schema.drop() subject.schema.drop() lab.schema.drop() ``` workflow-calcium-imaging Click to expand details ``` from workflow_calcium_imaging.pipeline import * imaging.schema.drop() scan.schema.drop() session.schema.drop() subject.schema.drop() lab.schema.drop() ``` workflow-miniscope Click to expand details ``` from workflow_miniscope.pipeline import * miniscope.schema.drop() session.schema.drop() subject.schema.drop() lab.schema.drop() ```","title":"Optionally drop all schemas"},{"location":"usage/install/#run-integration-tests","text":"Download the test dataset to your local machine. Note the directory where the dataset is saved (e.g. /tmp/testset ). Create an .env file within the docker directory with the following content. Replace /tmp/testset with the directory where you have the test dataset downloaded. TEST_DATA_DIR=/tmp/testset If testing an unreleased version of the element or your fork of an element or the workflow , within the Dockerfile uncomment the lines from the different options presented. This will allow you to install the repositories of interest and run the integration tests on those packages. Be sure that the element package version matches the version in the requirements.txt of the workflow . Run the Docker container. docker-compose -f ./docker/docker-compose-test.yaml up --build","title":"Run integration tests"}]}